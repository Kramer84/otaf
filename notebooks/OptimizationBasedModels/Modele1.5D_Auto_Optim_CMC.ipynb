{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecb329a-8614-42a5-82a8-78a3d5dacada",
   "metadata": {},
   "source": [
    "# Tolerance analysis of simple 1.5D model, automatic dictionary construction, crude monte carlo, optimizaiton on epistemic space of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd254d60-fd5b-4c8e-9f4a-1de5de524e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pprint\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import scipy\n",
    "import openturns as ot\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh as tr\n",
    "\n",
    "from math import pi, sqrt\n",
    "from joblib import Parallel, delayed\n",
    "from importlib import reload\n",
    "from IPython.display import display, clear_output, HTML, IFrame\n",
    "from time import time, sleep\n",
    "from sympy.printing import latex\n",
    "from trimesh import viewer as trview\n",
    "from scipy.optimize import OptimizeResult, minimize, Bounds, LinearConstraint, shgo, basinhopping, direct, dual_annealing\n",
    "\n",
    "import otaf\n",
    "#from efficient_kan import KAN, KANLinear\n",
    "\n",
    "notebook_name = os.path.splitext(os.path.basename(os.environ.get(\"JPY_SESSION_NAME\")))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8275252e-374d-44e2-b204-8764502e12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different measures of our problem\n",
    "X1 = 99.8   # Nominal Length of the male piece\n",
    "X2 = 100.0  # Nominal Length of the female piece\n",
    "X3 = 10.0   # Nominal height of the pieces\n",
    "t = 0.2*sqrt(2)    # Tolerance for X1 and X2. (95% conform)  (= t/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d5ac3-79f9-47d1-8329-ba5dce619632",
   "metadata": {},
   "source": [
    "## Coordinates, points, feature definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c637ce58-ba04-4d7d-a03f-441b1d8e60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global coordinate system\n",
    "R0 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "x_, y_, z_ = R0[0], R0[1], R0[2]\n",
    "\n",
    "# Important points\n",
    "# Pièce 1 (male)\n",
    "P1A0, P1A1, P1A2 = (\n",
    "    np.array((0, X3 / 2, 0.0)),\n",
    "    np.array((0, X3, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    ")\n",
    "P1B0, P1B1, P1B2 = (\n",
    "    np.array((X1, X3 / 2, 0.0)),\n",
    "    np.array((X1, X3, 0.0)),\n",
    "    np.array((X1, 0, 0.0)),\n",
    ")\n",
    "P1C0, P1C1, P1C2 = (\n",
    "    np.array((X1 / 2, 0, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    "    np.array((X1, 0, 0.0)),\n",
    ")\n",
    "\n",
    "# Pièce 2 (femelle)  # On met les points à hM et pas hF pour qu'ils soient bien opposées! (Besoin??)\n",
    "P2A0, P2A1, P2A2 = (\n",
    "    np.array((0, X3 / 2, 0.0)),\n",
    "    np.array((0, X3, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    ")\n",
    "P2B0, P2B1, P2B2 = (\n",
    "    np.array((X2, X3 / 2, 0.0)),\n",
    "    np.array((X2, X3, 0.0)),\n",
    "    np.array((X2, 0, 0.0)),\n",
    ")\n",
    "P2C0, P2C1, P2C2 = (\n",
    "    np.array((X2 / 2, 0, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    "    np.array((X2, 0, 0.0)),\n",
    ")\n",
    "\n",
    "# Local coordinate systems\n",
    "# Pièce1\n",
    "RP1a = np.array([-1 * x_, -1 * y_, z_])\n",
    "RP1b = R0\n",
    "RP1c = np.array([-y_, x_, z_])\n",
    "\n",
    "# Pièce2\n",
    "RP2a = R0\n",
    "RP2b = np.array([-1 * x_, -1 * y_, z_])\n",
    "RP2c = np.array([y_, -1 * x_, z_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed640e-40fc-48fe-ab9a-b46b5fd97b61",
   "metadata": {},
   "source": [
    "### Construction of the augmented system data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1248a3-5053-4030-b165-c48863d8d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_data = {\n",
    "    \"PARTS\" : {\n",
    "        '1' : {\n",
    "            \"a\" : {\n",
    "                \"FRAME\": RP1a,\n",
    "                \"POINTS\": {'A0' : P1A0, 'A1' : P1A1, 'A2' : P1A2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P2a'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"], # In this modelization, only defects on the right side\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"b\" : {\n",
    "                \"FRAME\": RP1b,\n",
    "                \"POINTS\": {'B0' : P1B0, 'B1' : P1B1, 'B2' : P1B2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P2b'],\n",
    "                \"CONSTRAINTS_D\": [\"NONE\"],\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"c\" : {\n",
    "                \"FRAME\": RP1c,\n",
    "                \"POINTS\": {'C0' : P1C0, 'C1' : P1C1, 'C2' : P1C2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P2c'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"],\n",
    "                \"CONSTRAINTS_G\": [\"SLIDING\"],            \n",
    "            },\n",
    "        },\n",
    "        '2' : {\n",
    "            \"a\" : {\n",
    "                \"FRAME\": RP2a,\n",
    "                \"POINTS\": {'A0' : P2A0, 'A1' : P2A1, 'A2' : P2A2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P1a'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"], # In this modelization, only defects on the right side\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"b\" : {\n",
    "                \"FRAME\": RP2b,\n",
    "                \"POINTS\": {'B0' : P2B0, 'B1' : P2B1, 'B2' : P2B2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P1b'],\n",
    "                \"CONSTRAINTS_D\": [\"NONE\"],\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"c\" : {\n",
    "                \"FRAME\": RP2c,\n",
    "                \"POINTS\": {'C0' : P2C0, 'C1' : P2C1, 'C2' : P2C2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P1c'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"],\n",
    "                \"CONSTRAINTS_G\": [\"SLIDING\"],            \n",
    "            },\n",
    "        }  \n",
    "    },\n",
    "    \"LOOPS\": {\n",
    "        \"COMPATIBILITY\": {\n",
    "            \"L0\": \"P1cC0 -> P2cC0 -> P2aA0 -> P1aA0\",\n",
    "            \"L1\": \"P1cC0 -> P2cC0 -> P2bB0 -> P1bB0\",\n",
    "        },\n",
    "    },\n",
    "    \"GLOBAL_CONSTRAINTS\": \"2D_NZ\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda4b960-047d-47fa-a3ae-0a131bef54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDA = otaf.AssemblyDataProcessor(system_data)\n",
    "SDA.generate_expanded_loops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57d0b7ba-4d97-410f-8ebc-84eacfc537b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLH = otaf.CompatibilityLoopHandling(SDA)\n",
    "compatibility_expressions = CLH.get_compatibility_expression_from_FO_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3e9b9d-0911-47ac-b5d2-156b71cd9de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The two interacting surfaces P1c and P2c are either not parallel or not facing each other.\n",
      "WARNING:root:The two interacting surfaces P2c and P1c are either not parallel or not facing each other.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing part 2, surface a for plane-to-plane interactions.\n",
      "usedGMatDat [['2', 'a', 'A0', '1', 'a', 'A0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['2', 'a', 'A2', '1', 'a', 'A2'], ['2', 'a', 'A1', '1', 'a', 'A1']]\n",
      "Found 2 unused gap matrices.\n",
      "Generated 2 interaction matrix loops for current matching.\n",
      "Processing part 2, surface b for plane-to-plane interactions.\n",
      "usedGMatDat [['2', 'b', 'B0', '1', 'b', 'B0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['2', 'b', 'B1', '1', 'b', 'B1'], ['2', 'b', 'B2', '1', 'b', 'B2']]\n",
      "Found 2 unused gap matrices.\n",
      "Generated 2 interaction matrix loops for current matching.\n"
     ]
    }
   ],
   "source": [
    "ILH = otaf.InterfaceLoopHandling(SDA, CLH, circle_resolution=20)\n",
    "interface_constraints = ILH.get_interface_loop_expressions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355ab7b1-7614-43b7-b413-d0f60c7eec6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [u_d_4, gamma_d_4, u_d_5, gamma_d_5]\n"
     ]
    }
   ],
   "source": [
    "SOCAM = otaf.SystemOfConstraintsAssemblyModel(\n",
    "    compatibility_expressions, interface_constraints\n",
    ")\n",
    "\n",
    "SOCAM.embedOptimizationVariable()\n",
    "\n",
    "print(len(SOCAM.deviation_symbols), SOCAM.deviation_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18adf2-2501-4ab0-a3d4-d02d6ff30ed5",
   "metadata": {},
   "source": [
    "## Construction of the stochastic model of the defects. (old lambda approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "296e3160-d1fe-4f84-ab1b-804aac69c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cm = 1.0\n",
    "sigma_e_pos = t / (6 * Cm)\n",
    "\n",
    "# Le défaut en orientation est piloté par une incertitude sur un angle. On suppose les angles petits << 1 rad\n",
    "theta_max = t / X3\n",
    "sigma_e_theta = (2*theta_max) / (6*Cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ba9590-7460-496d-8c15-f1979d39e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandDeviationVect = otaf.distribution.get_composed_normal_defect_distribution(\n",
    "    defect_names=SOCAM.deviation_symbols,\n",
    "    sigma_dict = {\"alpha\":sigma_e_theta, \n",
    "                  \"beta\":sigma_e_theta,\n",
    "                  \"gamma\":sigma_e_theta, \n",
    "                  \"u\":sigma_e_pos, \n",
    "                  \"v\":sigma_e_pos, \n",
    "                  \"w\":sigma_e_pos})\n",
    "\n",
    "cons, linearConstraint = otaf.optimization.lambda_constraint_dict_from_composed_distribution(RandDeviationVect)\n",
    "bounds_lambda = otaf.optimization.bounds_from_composed_distribution(RandDeviationVect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6e791-e715-454a-b51b-27cf1704b1e0",
   "metadata": {},
   "source": [
    "## Construction of a neural network based surrogate \n",
    "(could be omitted but makes things faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846e592-66b5-4f84-8220-e6b6818dd844",
   "metadata": {},
   "source": [
    "#### First generate the training sample :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "661bb8f1-85e1-4386-8e2a-a95e5645a2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728dd3fb1763443a8a0554c99a50b364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'STORAGE/training_sample_100000_seed_420_Modele1.5D_Auto_Optim_CMC_ai.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 37\u001b[0m\n\u001b[1;32m     25\u001b[0m TRAIN_RESULTS \u001b[38;5;241m=\u001b[39m otaf\u001b[38;5;241m.\u001b[39muncertainty\u001b[38;5;241m.\u001b[39mcompute_gap_optimizations_on_sample_batch(\n\u001b[1;32m     26\u001b[0m     SOCAM,\n\u001b[1;32m     27\u001b[0m     TRAIN_SAMPLE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#TRAIN_RESULTS = np.array([res.x[-1] for res in TRAIN_RESULTS],dtype=\"float32\") #Only s variable.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save the sample and results\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msample_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     38\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(file, TRAIN_SAMPLE)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(results_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/anaconda3/envs/otaf_test_env/lib/python3.13/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'STORAGE/training_sample_100000_seed_420_Modele1.5D_Auto_Optim_CMC_ai.npy'"
     ]
    }
   ],
   "source": [
    "# Define the seed, sample size, and file paths\n",
    "SEED = 420  # Example seed value\n",
    "sample_size = 100000\n",
    "model_name = notebook_name\n",
    "sample_filename = f'STORAGE/training_sample_{sample_size}_seed_{SEED}_{model_name}_ai.npy'\n",
    "results_filename = f'STORAGE/training_results_{sample_size}_seed_{SEED}_{model_name}_ai.npy'\n",
    "\n",
    "# Ensure reproducibility by setting the seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Check if the sample and results files already exist\n",
    "if os.path.exists(sample_filename) and os.path.exists(results_filename):\n",
    "    with open(sample_filename, 'rb') as file:\n",
    "        TRAIN_SAMPLE = np.load(file)\n",
    "    with open(results_filename, 'rb') as file:\n",
    "        TRAIN_RESULTS = np.load(file)\n",
    "    print(\"Loaded existing sample and results from file.\")\n",
    "else:\n",
    "    # Generate the sample\n",
    "    dist = otaf.distribution.multiply_composed_distribution_with_constant(\n",
    "        RandDeviationVect, 1.15) # We now work with low failure probabilities\n",
    "    #TRAIN_SAMPLE = np.array(otaf.uncertainty.generateLHSExperiment(dist, sample_size))\n",
    "    TRAIN_SAMPLE = np.array(dist.getSample(sample_size),dtype=\"float32\")\n",
    "    # Compute the results\n",
    "    TRAIN_RESULTS = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "        SOCAM,\n",
    "        TRAIN_SAMPLE,\n",
    "        bounds=None,\n",
    "        n_cpu=-2,\n",
    "        progress_bar=True,\n",
    "        batch_size=500,\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    #TRAIN_RESULTS = np.array([res.x[-1] for res in TRAIN_RESULTS],dtype=\"float32\") #Only s variable.\n",
    "    \n",
    "    # Save the sample and results\n",
    "    with open(sample_filename, 'wb') as file:\n",
    "        np.save(file, TRAIN_SAMPLE)\n",
    "    with open(results_filename, 'wb') as file:\n",
    "        np.save(file, TRAIN_RESULTS)\n",
    "    print(\"Generated and saved new sample and results with seed.\")\n",
    "\n",
    "# Assign X and y from TRAIN_SAMPLE and TRAIN_RESULTS\n",
    "Xtrain = TRAIN_SAMPLE\n",
    "ytrain = TRAIN_RESULTS\n",
    "print(f\"Ratio of failed simulations in sample : {np.where(ytrain[:,-1]<0,1,0).sum()/sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82716e8f-1d74-4ace-ab6c-7baded11854b",
   "metadata": {},
   "source": [
    "#### Then train the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954e704-8bfd-4f23-96ba-44692ecac705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_path = f'STORAGE/{notebook_name}.pth'\n",
    "load = False\n",
    "dim = int(RandDeviationVect.getDimension())\n",
    "neural_model = otaf.surrogate.NeuralRegressorNetwork(\n",
    "    dim, 1,\n",
    "    Xtrain, ytrain[:,-1], \n",
    "    clamping=True, \n",
    "    finish_critertion_epoch=5,\n",
    "    loss_finish=1e-6, \n",
    "    metric_finish=0.99999, \n",
    "    max_epochs=500, \n",
    "    batch_size=30000, \n",
    "    compile_model=False, \n",
    "    train_size=0.6, \n",
    "    save_path = save_path,\n",
    "    input_description=RandDeviationVect.getDescription(),\n",
    "    display_progress_disable=False)\n",
    "\n",
    "lr=0.003\n",
    "\n",
    "#neural_model.model = KAN([dim, 8, 4, 1])  #otaf.surrogate.get_base_relu_mlp_model(dim, 1, False)\n",
    "\n",
    "neural_model.model = otaf.torch.nn.Sequential(\n",
    "    *otaf.surrogate.get_custom_mlp_layers([dim, 100, 70, 30, 1], activation_class=otaf.torch.nn.GELU)\n",
    ")\n",
    "\n",
    "neural_model.optimizer = otaf.torch.optim.AdamW(neural_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "otaf.surrogate.initialize_model_weights(neural_model)\n",
    "neural_model.scheduler =  otaf.torch.optim.lr_scheduler.ExponentialLR(neural_model.optimizer, 1.0001)\n",
    "neural_model.loss_fn = otaf.torch.nn.MSELoss()\n",
    "#neural_model.loss_fn = otaf.uncertainty.LimitSpaceFocusedLoss(0.0001, 2, square=True) # otaf.uncertainty.PositiveLimitSpaceFocusedLoss(0.0001, 2, 4, square=False)\n",
    "\n",
    "if os.path.exists(save_path) and load:\n",
    "    neural_model.load_model()\n",
    "else :\n",
    "    neural_model.train_model()\n",
    "    neural_model.plot_results()\n",
    "    neural_model.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69344ac5-4131-45b1-afb5-700e3cee16ee",
   "metadata": {},
   "source": [
    "## Optimization on the imprecise space of defects, to get upper and lower probability of failure given the constraints on the defect parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7931fd9-0536-4ab4-81c8-cebb80edc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold and scaling factors\n",
    "scale_factor = 1e6  # Adjust this scaling factor for your specific range\n",
    "\n",
    "\n",
    "N_SAMPLE_MINI = int(1e7)\n",
    "standards = [RandDeviationVect.getParameter()[i] for i , param in enumerate(RandDeviationVect.getParameterDescription()) if \"sigma\" in param] \n",
    "means = [RandDeviationVect.getParameter()[i] for i , param in enumerate(RandDeviationVect.getParameterDescription()) if \"mu\" in param] \n",
    "sample = np.array(RandDeviationVect.getSample(N_SAMPLE_MINI))\n",
    "threshold = 0\n",
    "\n",
    "def model(x):\n",
    "    # Direct model without ai\n",
    "    gap_variable_array = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "        SOCAM, x, n_cpu=-1, progress_bar=True\n",
    "    )\n",
    "    slack_variable = gap_variable_array[:, -1]\n",
    "    return slack_variable\n",
    "\n",
    "def model2(x): \n",
    "    # Surrogate ai model\n",
    "    return np.squeeze(neural_model.evaluate_model_non_standard_space(x).detach().numpy())\n",
    "\n",
    "def optimization_function_mini(x, getJac=True, model=model2, scale_factor=scale_factor): \n",
    "    if getJac:\n",
    "        res = otaf.uncertainty.monte_carlo_non_compliancy_rate_w_gradient(\n",
    "            threshold, sample, means, standards, model, model_is_bool=True)(x)\n",
    "        # Scale both objective and Jacobian\n",
    "        return res[0] * scale_factor, res[1] * scale_factor\n",
    "    else:\n",
    "        x = sample * np.sqrt(x[np.newaxis, :])\n",
    "        return model(x).mean() * scale_factor  # Scale mean value\n",
    "\n",
    "def optimization_function_maxi(x, getJac=True, model=model2, scale_factor=scale_factor): \n",
    "    if getJac:\n",
    "        res = otaf.uncertainty.monte_carlo_non_compliancy_rate_w_gradient(\n",
    "            threshold, sample, means, standards, model, model_is_bool=True)(x)\n",
    "        # Scale both objective and Jacobian, but remember this is maximization, so we multiply by -1\n",
    "        return -1 * res[0] * scale_factor, -1 * res[1] * scale_factor\n",
    "    else:\n",
    "        x = sample * np.sqrt(x[np.newaxis, :])\n",
    "        return -1 * model(x).mean() * scale_factor  # Scale mean value\n",
    "\n",
    "# Define the callback function\n",
    "def print_callback(xk):\n",
    "    print(f\"Current parameter values: {xk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39f281-b637-4139-993e-a6387699ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(optimization_function_maxi,[0.3, 0.7]*2, \n",
    "         jac=True, method=\"SLSQP\", args=(True,),\n",
    "         options={\"disp\":True, \"maxiter\":100, \"ftol\":1e-6, 'eps':0.1},      \n",
    "         bounds=bounds_lambda, constraints=cons, callback=print_callback)\n",
    "\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed2b47-ac33-40fe-9438-ed2f584b475e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(optimization_function_mini,[1, 1, 1, 1], \n",
    "         jac=False, method=\"SLSQP\", args=(False,),\n",
    "         options={\"disp\":True, \"maxiter\":100, \"ftol\":1e-8, \"eps\":0.1},      \n",
    "         bounds=bounds_lambda, constraints=cons, callback=print_callback)\n",
    "\n",
    "display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc799553-c25a-47ce-bf8c-4ec60ee631b9",
   "metadata": {},
   "source": [
    "#### As you can see, the optimization is not capable of finding the minimum/maximum, due to the middle zone being flat. \n",
    "\n",
    "## Let's try global optimization !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e4e7c-709e-43d9-8585-893b3892de26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basinhopping for the maximization function\n",
    "x0_maxi = [.5]*4  # Initial guess\n",
    "\n",
    "minimizer_kwargs_maxi = {\n",
    "    \"method\": \"SLSQP\",\n",
    "    \"args\": (False,),\n",
    "    \"constraints\": cons,\n",
    "    \"bounds\": bounds_lambda,\n",
    "    \"options\": {\"disp\": False, \"maxiter\": 100, \"ftol\": 1e-6, \"eps\":0.01}\n",
    "}\n",
    "\n",
    "res_maxi = basinhopping(optimization_function_maxi, x0_maxi, \n",
    "                        niter=80, \n",
    "                        T=5.5, \n",
    "                        stepsize=2.3, \n",
    "                        niter_success=19,\n",
    "                        interval=15,\n",
    "                        target_accept_rate=0.44,\n",
    "                        stepwise_factor=0.73,                        \n",
    "                        minimizer_kwargs=minimizer_kwargs_maxi, disp=True)\n",
    "\n",
    "\n",
    "print(\"Maximization Result:\")\n",
    "print(res_maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e97e3a-0d2e-42da-9f22-f51b91c23566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basinhopping for the minimization function\n",
    "x0_mini = [.5]*4  # Initial guess\n",
    "\n",
    "minimizer_kwargs_mini = {\n",
    "    \"method\": \"SLSQP\",\n",
    "    \"args\": (False,),\n",
    "    \"constraints\": cons,\n",
    "    \"bounds\": bounds_lambda,\n",
    "    \"options\": {\"disp\": True, \"maxiter\": 100, \"ftol\": 1e-8, \"eps\":0.001}\n",
    "}\n",
    "\n",
    "res_mini = basinhopping(optimization_function_mini, x0_mini,\n",
    "                        niter=80, \n",
    "                        T=5.5, \n",
    "                        stepsize=2.3, \n",
    "                        niter_success=19,\n",
    "                        interval=15,\n",
    "                        target_accept_rate=0.44,\n",
    "                        stepwise_factor=0.73,                        \n",
    "                        minimizer_kwargs=minimizer_kwargs_maxi, disp=True)\n",
    "\n",
    "print(\"Minimization Result:\")\n",
    "print(res_mini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01902c9-1349-489e-8339-4877fc752ab9",
   "metadata": {},
   "source": [
    "best_params_maxi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3788ab8a-d909-4fdc-a211-f834dc55dfba",
   "metadata": {},
   "source": [
    "best_params_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec7c56-1153-4291-92ba-8e1e06478823",
   "metadata": {},
   "source": [
    "### Results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642802c9-0bcb-403c-8063-be4d1ff8a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_array = np.array([[80.182,  1.276,  3.110, 19.326, 85.911, 0.445,  0.726],\n",
    "[55.866,  1.695,  1.252, 97.917, 11.467, 0.510 ,  0.580],\n",
    "[81.046,  7.024,  3.161, 52.917 , 22.954, 0.122,  0.564],\n",
    "[57.059,  7.767,  2.544, 64.469, 16.478, 0.424,  0.887],\n",
    "[77.686  ,  4.737,  1.854, 88.378, 13.230, 0.223,  0.892],\n",
    "[177.175,   5.441,   4.900,  82.017 , 15.701,   0.710,   0.777],\n",
    "[69.056,  3.553,  2.071, 29.702, 26.634 , 0.324,  0.957],\n",
    "[100.007,   5.310,   1.037,  96.541, 20.207 ,   0.835,   0.982],\n",
    "[55.337,  5.494,  0.848, 39.742, 23.283 , 0.279 ,  0.731],\n",
    "[122.673,   4.215 ,   3.807,  38.694, 15.465,   0.339,   0.606],\n",
    "[95.877,  5.579,  0.654, 66.933, 25.929, 0.406,  0.513],\n",
    "[167.494,   8.662,   2.578 ,  63.197, 12.251,   0.830,   0.806],\n",
    "[50.299,  6.270,  0.807, 53.847, 66.134, 0.449,  0.690],\n",
    "[176.291,   5.938,   3.781,  52.256, 16.557,   0.728,   0.513]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4dc5c-2082-4c82-9d17-f3ea4c990e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = results_array\n",
    "# Extract bounds for normalization\n",
    "bounds = np.array(list(param_bounds.values()))\n",
    "min_bounds = bounds[:, 0]\n",
    "max_bounds = bounds[:, 1]\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = (data - min_bounds) / (max_bounds - min_bounds)\n",
    "\n",
    "# Calculate the median and IQR\n",
    "median_normalized = np.median(normalized_data, axis=0)\n",
    "q1_normalized = np.percentile(normalized_data, 25, axis=0)\n",
    "q3_normalized = np.percentile(normalized_data, 75, axis=0)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "for i, row in enumerate(normalized_data):\n",
    "    ax.plot(range(1, len(row) + 1), row, marker='o', label=f'Trajectory {i + 1}')\n",
    "\n",
    "# Plot the median\n",
    "ax.plot(range(1, len(median_normalized) + 1), median_normalized, color='black', linewidth=2, label='Median')\n",
    "\n",
    "# Plot the IQR as a see-through gray area\n",
    "ax.fill_between(range(1, len(median_normalized) + 1),\n",
    "                q1_normalized,\n",
    "                q3_normalized,\n",
    "                color='gray', alpha=0.3, label='Interquartile Range')\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Parameter Index')\n",
    "ax.set_ylabel('Normalized Value')\n",
    "ax.set_title('Normalized Parameter Trajectories from LHS Runs')\n",
    "ax.set_xticks(range(1, len(param_bounds) + 1))\n",
    "ax.set_xticklabels(param_bounds.keys(), rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0edfac8-9c65-41c6-9b65-f3211b631b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(data,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe61b98-e79b-46a5-a937-5e6eb8e8f060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
