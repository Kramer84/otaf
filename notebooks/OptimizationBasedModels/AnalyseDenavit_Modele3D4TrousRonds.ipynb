{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872744ae-a8b8-4796-9eff-ecbae30ae4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pprint\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import openturns as ot\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh as tr\n",
    "from importlib import reload\n",
    "from functools import partial\n",
    "\n",
    "from math import pi\n",
    "from joblib import Parallel, delayed\n",
    "from importlib import reload\n",
    "from IPython.display import display, clear_output\n",
    "from time import time\n",
    "from sympy.printing import latex\n",
    "from trimesh import viewer as trview\n",
    "import sklearn\n",
    "\n",
    "from scipy.optimize import OptimizeResult, minimize, basinhopping, differential_evolution, brute, shgo, check_grad, approx_fprime\n",
    "\n",
    "import tqdm\n",
    "import otaf\n",
    "\n",
    "from gldpy import GLD\n",
    "\n",
    "ot.Log.Show(ot.Log.NONE)\n",
    "np.set_printoptions(suppress=True)\n",
    "ar = np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817c72a-0818-4d16-a347-bc57ef3cf4d7",
   "metadata": {},
   "source": [
    "# Notebook for the analysis of a system comprised of N + 2 parts, 2 plates with N = N1 x N2 holes, and N pins. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f3fc3-09ea-4e9a-bca2-0a532e8398eb",
   "metadata": {},
   "source": [
    "### Defintion on global descriptive parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91253f14-bf22-495a-b2da-aa89d49d970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NX = 2 ## Number of holes on x axis\n",
    "NY = 2 ## Number of holes on y axis\n",
    "Dext = 20 ## Diameter of holes in mm\n",
    "Dint = 19.8 ## Diameter of pins in mm\n",
    "EH = 50 ## Distance between the hole axises\n",
    "LB = 25 # Distance between border holes axis and edge.\n",
    "hPlate = 30 #Height of the plates in mm\n",
    "hPin = 60 #Height of the pins in mm\n",
    "\n",
    "CIRCLE_RESOLUTION = 16 # NUmber of points to model the contour of the outer holes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fa6a57-ff00-4438-9487-0304ddcf7cd9",
   "metadata": {},
   "source": [
    "### Defining and constructing the system data dictionary\n",
    "\n",
    "The plates have NX * NY + 1 surfaces. The lower left point has coordinate 0,0,0\n",
    "\n",
    "We only model the surfaces that are touching. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9be0ccd7-a39e-4eef-83c8-ff9d759040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PARTS = NX * NY * 2\n",
    "LX = (NX - 1) * EH + 2*LB\n",
    "LY = (NY - 1) * EH + 2*LB\n",
    "\n",
    "contour_points = ar([[0,0,0],[LX,0,0],[LX,LY,0],[0,LY,0]])\n",
    "\n",
    "R0 = ar([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "x_, y_, z_ = R0[0], R0[1], R0[2]\n",
    "\n",
    "Frame1 = ar([z_,y_,-x_])\n",
    "Frame2 = ar([-z_,y_,x_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42692f0e-e845-4efa-9470-805dc0d65b3b",
   "metadata": {},
   "source": [
    "First we define the base part dictionaries for the upper and lower plate, without holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a1bb46-a7f6-4d9d-82f0-07ceccb9ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_data = {\n",
    "    \"PARTS\" : {\n",
    "        '0' : {\n",
    "            \"a\" : {\n",
    "                \"FRAME\": Frame1,\n",
    "                \"POINTS\": {'A0' : ar([0,0,0]),\n",
    "                           'A1' : ar([LX,0,0]),\n",
    "                           'A2' : ar([LX,LY,0]),\n",
    "                           'A3' : ar([0,LY,0]),\n",
    "                        },\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P1a'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"],\n",
    "                \"CONSTRAINTS_G\": [\"SLIDING\"],            \n",
    "            }\n",
    "        },\n",
    "        '1' : {\n",
    "            \"a\" : {\n",
    "                \"FRAME\": Frame2,\n",
    "                \"POINTS\": {'A0' : ar([0,0,0]),\n",
    "                           'A1' : ar([LX,0,0]),\n",
    "                           'A2' : ar([LX,LY,0]),\n",
    "                           'A3' : ar([0,LY,0]),\n",
    "                        },\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P0a'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"],\n",
    "                \"CONSTRAINTS_G\": [\"SLIDING\"],            \n",
    "            }\n",
    "        }  \n",
    "    },\n",
    "    \"LOOPS\": {\n",
    "        \"COMPATIBILITY\": {\n",
    "        },\n",
    "    },\n",
    "    \"GLOBAL_CONSTRAINTS\": \"3D\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e81a47-6214-4774-ab58-8d16bd43ac9b",
   "metadata": {},
   "source": [
    "Then we iterate over the pin dimensions NX and NY, and create the corresponding holes and pins. At the same time there is 1 loop per pin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29491ebe-37b5-4879-a740-64db3434d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_gen = otaf.common.alphabet_generator()\n",
    "next(alpha_gen) # skipping 'a' as it has already been used above\n",
    "part_id = 2 # Start part index for pins\n",
    "for i in range(NX):\n",
    "    for j in range(NY):\n",
    "        pcor = ar([LB+i*EH, LB+j*EH, 0]) # Point coordinate for hole / pins\n",
    "        slab = next(alpha_gen) # Surface label, same for each mating pin so its easeir to track\n",
    "        # Creating pin\n",
    "        system_data[\"PARTS\"][str(part_id)] = {}\n",
    "        system_data[\"PARTS\"][str(part_id)][slab] = {\n",
    "            \"FRAME\": Frame1, # Frame doesn't really matter, as long as x is aligned on the axis\n",
    "            \"ORIGIN\": pcor, \n",
    "            \"TYPE\": \"cylinder\",\n",
    "            \"RADIUS\": Dint / 2,\n",
    "            \"EXTENT_LOCAL\": {\"x_max\": hPin/2, \"x_min\": -hPin/2},\n",
    "            \"INTERACTIONS\": [f\"P0{slab}\", f\"P1{slab}\"], \n",
    "            \"SURFACE_DIRECTION\": \"centrifugal\",\n",
    "            \"CONSTRAINTS_D\": [\"PERFECT\"], # No defects on the pins\n",
    "            \"BLOCK_ROTATIONS_G\": 'x', # The pins do not rotate around their axis\n",
    "            \"BLOCK_TRANSLATIONS_G\": 'x', # The pins do not slide along their axis\n",
    "        }\n",
    "        # Adding hole to part 0\n",
    "        system_data[\"PARTS\"][\"0\"][slab] = {\n",
    "            \"FRAME\": Frame1,\n",
    "            \"ORIGIN\": pcor, \n",
    "            \"TYPE\": \"cylinder\",\n",
    "            \"RADIUS\": Dext / 2,\n",
    "            \"EXTENT_LOCAL\": {\"x_max\": hPin/2, \"x_min\": -hPin/2},\n",
    "            \"INTERACTIONS\": [f\"P{part_id}{slab}\"], \n",
    "            \"SURFACE_DIRECTION\": \"centripetal\",\n",
    "        }\n",
    "        # Adding hole to part 1\n",
    "        system_data[\"PARTS\"][\"1\"][slab] = {\n",
    "            \"FRAME\": Frame2,\n",
    "            \"ORIGIN\": pcor, \n",
    "            \"TYPE\": \"cylinder\",\n",
    "            \"RADIUS\": Dext / 2,\n",
    "            \"EXTENT_LOCAL\": {\"x_max\": hPin/2, \"x_min\": -hPin/2},\n",
    "            \"INTERACTIONS\": [f\"P{part_id}{slab}\"],\n",
    "            \"SURFACE_DIRECTION\": \"centripetal\",\n",
    "        }\n",
    "        # Construct Compatibility loop\n",
    "        loop_id = f\"L{part_id-1}\"\n",
    "        formater = lambda i,l : f\"P{i}{l}{l.upper()}0\" \n",
    "        system_data[\"LOOPS\"][\"COMPATIBILITY\"][loop_id] = f\"P0aA0 -> {formater(0,slab)} -> {formater(part_id,slab)} -> {formater(1,slab)} -> P1aA0\"\n",
    "        part_id += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "293f981a-efe3-4b44-b9f3-b7a580cf3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDA = otaf.AssemblyDataProcessor(system_data)\n",
    "SDA.generate_expanded_loops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e26ae8-3e27-496d-ac65-9f4385d89105",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLH = otaf.CompatibilityLoopHandling(SDA)\n",
    "compatibility_expressions = CLH.get_compatibility_expression_from_FO_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87736588-7cec-4360-b748-2dfc7666461e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing part 0, surface b for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['0', 'b', 'B0', '2', 'b', 'B0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['0', 'b', 'B1', '2', 'b', 'B1'], ['0', 'b', 'B2', '2', 'b', 'B2']]\n",
      "Found 2 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP0bB0P2bB0 with GP0bB1P2bB1\n",
      "Matching used and unused gap matrices: GP0bB0P2bB0 with GP0bB2P2bB2\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n",
      "Processing part 0, surface c for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['0', 'c', 'C0', '3', 'c', 'C0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['0', 'c', 'C2', '3', 'c', 'C2'], ['0', 'c', 'C1', '3', 'c', 'C1']]\n",
      "Found 2 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP0cC0P3cC0 with GP0cC2P3cC2\n",
      "Matching used and unused gap matrices: GP0cC0P3cC0 with GP0cC1P3cC1\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n",
      "Processing part 0, surface d for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['0', 'd', 'D0', '4', 'd', 'D0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['0', 'd', 'D1', '4', 'd', 'D1'], ['0', 'd', 'D2', '4', 'd', 'D2']]\n",
      "Found 2 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP0dD0P4dD0 with GP0dD1P4dD1\n",
      "Matching used and unused gap matrices: GP0dD0P4dD0 with GP0dD2P4dD2\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n",
      "Processing part 0, surface e for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['0', 'e', 'E0', '5', 'e', 'E0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['0', 'e', 'E2', '5', 'e', 'E2'], ['0', 'e', 'E1', '5', 'e', 'E1']]\n",
      "Found 2 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP0eE0P5eE0 with GP0eE2P5eE2\n",
      "Matching used and unused gap matrices: GP0eE0P5eE0 with GP0eE1P5eE1\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n",
      "Processing part 1, surface a for plane-to-plane interactions.\n",
      "usedGMatDat [['1', 'a', 'A0', '0', 'a', 'A0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['1', 'a', 'A3', '0', 'a', 'A3'], ['1', 'a', 'A1', '0', 'a', 'A1'], ['1', 'a', 'A2', '0', 'a', 'A2']]\n",
      "Found 3 unused gap matrices.\n",
      "Generated 3 interaction matrix loops for current matching.\n",
      "Processing part 2, surface b for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['2', 'b', 'B0', '1', 'b', 'B0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['2', 'b', 'B2', '0', 'b', 'B2'], ['2', 'b', 'B1', '0', 'b', 'B1'], ['2', 'b', 'B1', '1', 'b', 'B2'], ['2', 'b', 'B2', '1', 'b', 'B1'], ['2', 'b', 'B0', '0', 'b', 'B0']]\n",
      "Found 5 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP2bB0P1bB0 with GP2bB1P1bB2\n",
      "Matching used and unused gap matrices: GP2bB0P1bB0 with GP2bB2P1bB1\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n",
      "Processing part 3, surface c for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['3', 'c', 'C0', '1', 'c', 'C0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['3', 'c', 'C0', '0', 'c', 'C0'], ['3', 'c', 'C2', '1', 'c', 'C1'], ['3', 'c', 'C1', '1', 'c', 'C2'], ['3', 'c', 'C1', '0', 'c', 'C1'], ['3', 'c', 'C2', '0', 'c', 'C2']]\n",
      "Found 5 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP3cC0P1cC0 with GP3cC2P1cC1\n",
      "Matching used and unused gap matrices: GP3cC0P1cC0 with GP3cC1P1cC2\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n",
      "Processing part 4, surface d for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['4', 'd', 'D0', '1', 'd', 'D0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['4', 'd', 'D1', '0', 'd', 'D1'], ['4', 'd', 'D2', '0', 'd', 'D2'], ['4', 'd', 'D0', '0', 'd', 'D0'], ['4', 'd', 'D2', '1', 'd', 'D1'], ['4', 'd', 'D1', '1', 'd', 'D2']]\n",
      "Found 5 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP4dD0P1dD0 with GP4dD2P1dD1\n",
      "Matching used and unused gap matrices: GP4dD0P1dD0 with GP4dD1P1dD2\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n",
      "Processing part 5, surface e for cylinder-to-cylinder interactions.\n",
      "usedGMatDat [['5', 'e', 'E0', '1', 'e', 'E0']]\n",
      "Found 1 used gap matrices.\n",
      "unusedGMatDat [['5', 'e', 'E0', '0', 'e', 'E0'], ['5', 'e', 'E1', '1', 'e', 'E2'], ['5', 'e', 'E2', '1', 'e', 'E1'], ['5', 'e', 'E1', '0', 'e', 'E1'], ['5', 'e', 'E2', '0', 'e', 'E2']]\n",
      "Found 5 unused gap matrices.\n",
      "Matching used and unused gap matrices: GP5eE0P1eE0 with GP5eE1P1eE2\n",
      "Matching used and unused gap matrices: GP5eE0P1eE0 with GP5eE2P1eE1\n",
      "Generated 32 interaction equations for current matching.\n",
      "Total interaction equations generated: 32\n"
     ]
    }
   ],
   "source": [
    "ILH = otaf.InterfaceLoopHandling(SDA, CLH, circle_resolution=CIRCLE_RESOLUTION)\n",
    "interface_constraints = ILH.get_interface_loop_expressions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d2f4ba-f4bd-425a-ba0c-50cabc500272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 [v_d_0, w_d_0, beta_d_0, gamma_d_0, v_d_2, w_d_2, beta_d_2, gamma_d_2, v_d_5, w_d_5, beta_d_5, gamma_d_5, v_d_7, w_d_7, beta_d_7, gamma_d_7, v_d_8, w_d_8, beta_d_8, gamma_d_8, v_d_10, w_d_10, beta_d_10, gamma_d_10, v_d_11, w_d_11, beta_d_11, gamma_d_11, v_d_13, w_d_13, beta_d_13, gamma_d_13]\n"
     ]
    }
   ],
   "source": [
    "SOCAM = otaf.SystemOfConstraintsAssemblyModel(\n",
    "    compatibility_expressions, interface_constraints\n",
    ")\n",
    "\n",
    "SOCAM.embedOptimizationVariable()\n",
    "\n",
    "print(len(SOCAM.deviation_symbols), SOCAM.deviation_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be834b-fc7d-4fc3-85eb-2d8768c446ed",
   "metadata": {},
   "source": [
    "## Construction of the stochastic model of the defects. (old lambda approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31049a41-8b2a-49c0-8142-2819f94c51ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.1 * np.sqrt(2)\n",
    "Cm = 1  # Process capability\n",
    "\n",
    "# Defining the uncertainties on the position and orientation uncertainties.\n",
    "sigma_e_pos = tol / (6 * Cm)\n",
    "theta_max = tol / hPlate\n",
    "sigma_e_theta = (2 * theta_max) / (6 * Cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449f3658-2055-4cd6-a9bb-0ef7d520204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandDeviationVect = otaf.distribution.get_composed_normal_defect_distribution(\n",
    "    defect_names=SOCAM.deviation_symbols,\n",
    "    sigma_dict = {\"alpha\":sigma_e_theta, \n",
    "                  \"beta\":sigma_e_theta,\n",
    "                  \"gamma\":sigma_e_theta, \n",
    "                  \"u\":sigma_e_pos, \n",
    "                  \"v\":sigma_e_pos, \n",
    "                  \"w\":sigma_e_pos})\n",
    "dim_devs = int(RandDeviationVect.getDimension())\n",
    "\n",
    "def get_uniform_from_deviation_vect(composed_distribution, coef=2):\n",
    "    \"\"\"To construct a composed distribution of uniform distributions from\n",
    "    normal distributions to feed the ai cause it struggles. \n",
    "    \"\"\"\n",
    "    uni_dist_list = []\n",
    "    parameters = composed_distribution.getParametersCollection()\n",
    "    for i in range(len(parameters)-1):\n",
    "        #assert parameter[0]==0.0, \"We said 0 mean!\"\n",
    "        std = parameters[i][1] #We assume all parameters are normal distributions!!\n",
    "        a = -coef*std\n",
    "        b = coef*std\n",
    "        uni_dist_list.append(ot.Uniform(a,b))\n",
    "    return ot.ComposedDistribution(uni_dist_list)\n",
    "\n",
    "UniRandDevVect = get_uniform_from_deviation_vect(RandDeviationVect, 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a0a59-111e-461e-b796-afacf7adce72",
   "metadata": {},
   "source": [
    "## Construction of a neural network based surrogate \n",
    "(could be omitted but makes things faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39b69ae0-7fd0-417f-9e3f-53e41468eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from STORAGE/data_1000000_add_100000_seed_420_model2x2Pins_coeff_1.41_2.00_3.00_3.50.npz.\n",
      "Ratio of failed simulations in sample: 0.4486017443334516\n"
     ]
    }
   ],
   "source": [
    "# Define the seed, sample size, and coefficients\n",
    "SEED = 420  # Example seed value\n",
    "sample_size = 1000000\n",
    "additional_sample_size = 100000  # Number of additional permutation-based samples\n",
    "scaling_coefficients = [np.sqrt(2), 2, 3, 3.5]  # Dynamic coefficients\n",
    "model_name = f\"model{NX}x{NY}Pins\"\n",
    "\n",
    "# Create a unique file name based on seed, sample size, and coefficients\n",
    "coeff_str = \"_\".join([f\"{coeff:.2f}\" for coeff in scaling_coefficients])\n",
    "data_filename = f'STORAGE/data_{sample_size}_add_{additional_sample_size}_seed_{SEED}_{model_name}_coeff_{coeff_str}.npz'\n",
    "\n",
    "# Check if the data file already exists\n",
    "if os.path.exists(data_filename):\n",
    "    # Load the sample and results from the file\n",
    "    with np.load(data_filename) as data:\n",
    "        Xtrain = data['Xtrain']\n",
    "        ytrain = data['ytrain']\n",
    "    print(f\"Loaded data from {data_filename}.\")\n",
    "else:\n",
    "    # Generate the base sample if it doesn't exist\n",
    "    np.random.seed(SEED)  # Ensure reproducibility\n",
    "    dist = UniRandDevVect\n",
    "    TRAIN_SAMPLE = otaf.sampling.generate_and_transform_sequence(dist.getDimension(), sample_size, dist, sequence_type='halton')\n",
    "    TRAIN_SAMPLE = np.array(TRAIN_SAMPLE, dtype=\"float32\") #dist.getSample(sample_size), dtype=\"float32\")\n",
    "\n",
    "    # Compute the results for the base sample\n",
    "    TRAIN_RESULTS = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "        SOCAM,\n",
    "        TRAIN_SAMPLE,\n",
    "        bounds=None,\n",
    "        n_cpu=-2,\n",
    "        progress_bar=True,\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "\n",
    "    # Generate permutations and compute their results using the scaling coefficients\n",
    "    permutation_samples = []\n",
    "    additional_results = []\n",
    "\n",
    "    indices, subgroup_sizes = SOCAM.get_feature_indices_and_dimensions()\n",
    "    _, std_deviations = otaf.distribution.get_means_standards_composed_distribution(RandDeviationVect)\n",
    "    std_deviations = np.array(std_deviations, dtype=\"float32\")\n",
    "\n",
    "    for coeff in scaling_coefficients:\n",
    "        # Generate permutation samples for each coefficient\n",
    "        samples = otaf.sampling.generate_scaled_permutations(subgroup_sizes, std_deviations, additional_sample_size) * coeff\n",
    "        permutation_samples.append(samples)\n",
    "\n",
    "        # Compute the results for each permutation sample\n",
    "        results = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "            SOCAM, samples, bounds=None,\n",
    "            n_cpu=-2, progress_bar=True, dtype=\"float32\"\n",
    "        )\n",
    "        additional_results.append(results)\n",
    "\n",
    "    # Concatenate the generated permutation samples and results to the original ones\n",
    "    TRAIN_SAMPLE = np.vstack([TRAIN_SAMPLE] + permutation_samples)\n",
    "    TRAIN_RESULTS = np.vstack([TRAIN_RESULTS] + additional_results)\n",
    "\n",
    "    # Convert to final training arrays\n",
    "    Xtrain = np.array(TRAIN_SAMPLE, dtype=\"float32\")\n",
    "    ytrain = np.array(TRAIN_RESULTS, dtype=\"float32\")\n",
    "\n",
    "    # Save the generated sample and results to a single compressed file\n",
    "    np.savez_compressed(data_filename, Xtrain=Xtrain, ytrain=ytrain)\n",
    "    print(f\"Generated and saved data to {data_filename}.\")\n",
    "\n",
    "# Calculate failure/success ratios\n",
    "train_failure_ratio = np.where(ytrain[..., -1] < 0, 1, 0).sum() / len(ytrain)\n",
    "train_success_ratio = np.where(ytrain[..., -1] >= 0, 1, 0).sum() / len(ytrain)\n",
    "\n",
    "print(f\"Ratio of failed simulations in sample: {train_failure_ratio}\")\n",
    "\n",
    "out_dim = ytrain.shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a834f872-35a9-4ecd-a58d-8bfff199c95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing sample and results from file.\n",
      "Ratio of failed simulations in validation sample : 0.492868\n"
     ]
    }
   ],
   "source": [
    "# Define the seed, sample size, and file paths\n",
    "SEED_valid = 1366631  # Example seed value\n",
    "sample_size_valid = 250000\n",
    "model_name = f\"model{NX}x{NY}Pins\"\n",
    "sample_filename = f'STORAGE/validation_sample_{sample_size_valid}_seed_{SEED_valid}_{model_name}_ai.npy'\n",
    "results_filename = f'STORAGE/validation_results_{sample_size_valid}_seed_{SEED_valid}_{model_name}_ai.npy'\n",
    "\n",
    "# Ensure reproducibility by setting the seed\n",
    "np.random.seed(SEED_valid)\n",
    "\n",
    "if os.path.exists(sample_filename) and os.path.exists(results_filename):\n",
    "    with open(sample_filename, 'rb') as file:\n",
    "        VALID_SAMPLE = np.load(file)\n",
    "    with open(results_filename, 'rb') as file:\n",
    "        VALID_RESULTS = np.load(file)\n",
    "    print(\"Loaded existing sample and results from file.\")\n",
    "else:\n",
    "    # Generate the sample\n",
    "    dist = UniRandDevVect # otaf.uncertainty.multiply_composed_distribution_with_constant(RandDeviationVect, 1.05) # We now work with low failure probabilities\n",
    "    #VALID_SAMPLE = np.array(otaf.uncertainty.generateLHSExperiment(dist, sample_size_valid))\n",
    "    VALID_SAMPLE = np.array(dist.getSample(sample_size_valid),dtype=\"float32\")\n",
    "    # Compute the results\n",
    "    VALID_RESULTS = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "        SOCAM,\n",
    "        VALID_SAMPLE,\n",
    "        n_cpu=-2,\n",
    "        progress_bar=True,\n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    # Save the sample and results\n",
    "    with open(sample_filename, 'wb') as file:\n",
    "        np.save(file, VALID_SAMPLE)\n",
    "    with open(results_filename, 'wb') as file:\n",
    "        np.save(file, VALID_RESULTS)\n",
    "    print(\"Generated and saved new sample and results with seed for validation.\")\n",
    "\n",
    "# Assign X and y from VALID_SAMPLE and VALID_RESULTS\n",
    "Xvalid = VALID_SAMPLE\n",
    "yvalid = VALID_RESULTS\n",
    "print(f\"Ratio of failed simulations in validation sample : {np.where(yvalid[...,-1]<0,1,0).sum()/sample_size_valid}\")\n",
    "out_dim = yvalid.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9702a31-f291-4088-9c8b-3635a7e54e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first generate a classic LHS design of expezriment of size 16.\n",
    "D_lambd = len(SOCAM.deviation_symbols)\n",
    "lambda_vect_unconditioned = ot.ComposedDistribution([ot.Uniform(0, 1)] * D_lambd)\n",
    "lambda_vect_unconditioned.setDescription(list(map(str, SOCAM.deviation_symbols)))\n",
    "N_lambda = 800\n",
    "lambda_sample_unconditioned = otaf.sampling.generateLHSExperiment(lambda_vect_unconditioned ,N_lambda, 999)\n",
    "#lambda_sample_unconditioned = lambda_sample_unconditioned_generator.generate()\n",
    "lambda_sample_random = lambda_vect_unconditioned.getSample(N_lambda)\n",
    "lambda_sample_conditioned = otaf.sampling.condition_lambda_sample(lambda_sample_random, squared_sum=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8928375-66f3-4022-a282-8d9c37cb8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_next = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d884d4-d7a6-4247-afd4-f0eab178e33b",
   "metadata": {},
   "source": [
    "# *NOT IMPORTANT* The following was a brute force part to validate the results of the surrogate but takes 27h to run. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a58f7d-9016-4de7-9be7-fd47a983f8e2",
   "metadata": {},
   "source": [
    "Usually, in reliability, regression models are prefered to classifiers ref{https://www.sciencedirect.com/science/article/pii/S0167473020300989}, but in this case\n",
    "the performance of the regression wasn't precise enough, and a classifier with some tweaking provided excellent results for reducing the number of calls to the optimization function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a75b2d13-b9a4-409c-b9cc-88aaeed232e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not pass_next:    \n",
    "    bounds = None\n",
    "    SEED_MC_PF = 6436431\n",
    "    SIZE_MC_PF = int(1e6) #1e4\n",
    "    optimizations_array = np.empty((N_lambda, SIZE_MC_PF), dtype=OptimizeResult)\n",
    "    failure_probabilities, successes, s_values, statuses = [], [], [], []\n",
    "    failed_optimization_points = []\n",
    "    \n",
    "    start_time = time()  # Record the start time\n",
    "    for i in range(N_lambda):\n",
    "        print(f\"Doing iteration {i} of {N_lambda}\")\n",
    "        if i>0:\n",
    "            print(f\"Failure probability, Min: {min(failure_probabilities)}, / Max: {max(failure_probabilities)}\")\n",
    "            print(f\"Failed {(1-successes).sum()} optimizations on { SIZE_MC_PF}\")\n",
    "            print(\"s_mean: \", s_values.mean().round(3), \"s_min: \", np.nanmin(s_values).round(3), \"s_max: \", np.nanmax(s_values).round(3))\n",
    "            print(\"Statuses are:\", np.unique(statuses))\n",
    "        ot.RandomGenerator.SetSeed(SEED_MC_PF)\n",
    "        deviation_samples = np.array(RandDeviationVect.getSample(SIZE_MC_PF)) * np.array(\n",
    "            lambda_sample_conditioned[i]\n",
    "        )\n",
    "        optimizations = otaf.uncertainty.compute_gap_optimizations_on_sample(\n",
    "                SOCAM,\n",
    "                deviation_samples,\n",
    "                bounds=bounds,\n",
    "                n_cpu=-1,\n",
    "                progress_bar=True,\n",
    "            )\n",
    "        successes = np.array([opt.success for opt in optimizations], dtype=bool)\n",
    "        statuses = np.array([opt.status for opt in optimizations], dtype=int)\n",
    "        \n",
    "        if successes.sum() == 0:\n",
    "            print(\"All optimizations failed\")\n",
    "            sleep(0.5)\n",
    "    \n",
    "        failed_optimization_points.append(deviation_samples[np.invert(successes), :])\n",
    "        \n",
    "        s_values = np.array([opt.fun for opt in optimizations], dtype=float)\n",
    "        s_values = np.nan_to_num(s_values, nan=np.nanmax(s_values))*-1 # Cause the obj function C is -1*s and failed optimizations count as a negative s\n",
    "        failure_probabilities.append(np.where(s_values < 0, 1, 0).mean())\n",
    "        clear_output(wait=True)\n",
    "    print(f\"Done {len (lambda_sample_conditioned)} experiments.\")\n",
    "    print(f\"Elapsed time: {time() - start_time:.3f} seconds.\")\n",
    "    failed_optimization_points = np.vstack(failed_optimization_points)\n",
    "    \n",
    "    X = otaf.uncertainty.find_best_worst_quantile(np.array(lambda_sample_conditioned), np.array(failure_probabilities), 0.1)\n",
    "    (best_5p_lambda, best_5p_res), (worst_5p_lambda, worst_5p_res) = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd2dc71-b41e-41fb-b848-b1bb548f1a69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if not pass_next:    \n",
    "    best_worst_quants = otaf.uncertainty.find_best_worst_quantile(np.array(lambda_sample_conditioned)**2, np.array(failure_probabilities), 0.2)\n",
    "    (best_5p_lambda, best_5p_res), (worst_5p_lambda, worst_5p_res) = best_worst_quants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f941541-b297-43ea-904d-e4572cd050fa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if not pass_next:    \n",
    "    print(\"Lower probability of failure:\", \"{:.5e}\".format(min(failure_probabilities)))\n",
    "    print(\"Upper probability of failure:\", \"{:.5e}\".format(max(failure_probabilities)))\n",
    "    plt.hist(failure_probabilities)\n",
    "    plt.show()\n",
    "    otaf.plotting.plot_best_worst_results(best_5p_res, worst_5p_res, figsize=(10,5))\n",
    "    \n",
    "    variable_labels = [var for var in lambda_sample_conditioned.getDescription()]\n",
    "    otaf.plotting.plot_best_worst_input_data(best_5p_lambda, worst_5p_lambda, variable_labels, figsize=(20,5), labels=False)\n",
    "    #Upper probability of failure: 5.64000e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6afecc21-5985-42ba-9f7d-42ade61260c0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'failure_probabilities' is not defined\n",
      "passing\n"
     ]
    }
   ],
   "source": [
    "# Define the filename\n",
    "filename = 'STORAGE/Modele3D4TrousStored_objects.pkl'\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(filename):\n",
    "    # Load the data from the file\n",
    "    with open(filename, 'rb') as file:\n",
    "        loaded_data = pickle.load(file)\n",
    "        if 'lambda_sample_conditioned' not in globals():\n",
    "            lambda_sample_conditioned = loaded_data['lambda_sample_conditioned']\n",
    "        if 'failure_probabilities' not in globals():\n",
    "            failure_probabilities = loaded_data['failure_probabilities']\n",
    "        if 'successes' not in globals():\n",
    "            successes = loaded_data['successes']\n",
    "        if 's_values' not in globals():\n",
    "            s_values = loaded_data['s_values']\n",
    "        if 'statuses' not in globals():\n",
    "            statuses = loaded_data['statuses']\n",
    "        if 'failed_optimization_points' not in globals():\n",
    "            failed_optimization_points = loaded_data['failed_optimization_points']\n",
    "    print(f\"Data has been loaded from {filename}\")\n",
    "else:\n",
    "    try: \n",
    "        # Combine all objects into a dictionary for easier storage\n",
    "        data_to_store = {\n",
    "            'lambda_sample_conditioned': lambda_sample_conditioned,\n",
    "            'failure_probabilities': failure_probabilities,\n",
    "            'successes': successes,\n",
    "            's_values': s_values,\n",
    "            'statuses': statuses,\n",
    "            'failed_optimization_points': failed_optimization_points\n",
    "        }\n",
    "    \n",
    "        # Store the objects using pickle\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(data_to_store, file)\n",
    "        print(f\"Data has been stored in {filename}\")\n",
    "    except Exception as e :\n",
    "        print(e)\n",
    "        print(\"passing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59093057-137c-4b22-87d8-71e8700c45ef",
   "metadata": {},
   "source": [
    "# Training of the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1fdc6a0-4571-4cfc-a357-060eeadd789c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 392, 400, 312, 128]\n"
     ]
    }
   ],
   "source": [
    "load = True\n",
    "save_path = f'STORAGE/AI_MODEL_3D_DIM_{dim_devs}_BINARY_SLACK_CLASSIFIER.pth'\n",
    "binary_slack_classifier = otaf.surrogate.BinaryClassificationModel(\n",
    "    dim_devs, 2, \n",
    "    Xtrain, ytrain[:,-1], \n",
    "    slack_threshold=0.0,\n",
    "    clamping=True, \n",
    "    clamping_threshold=np.pi,\n",
    "    metric_finish=1e6,\n",
    "    max_epochs=500, \n",
    "    batch_size=100000,\n",
    "    train_size=0.70,\n",
    "    display_progress_disable=True,\n",
    "    squeeze_labels = True,\n",
    "    labels_to_long = True,\n",
    "    use_dual_target = False,\n",
    "    save_path = save_path)\n",
    "\n",
    "lr=0.005\n",
    "\n",
    "binary_slack_classifier.model = otaf.torch.nn.Sequential(\n",
    "    *otaf.surrogate.get_custom_mlp_layers([dim_devs, dim_devs*5], activation_class = otaf.torch.nn.LeakyReLU,),\n",
    "    *otaf.surrogate.get_custom_mlp_layers([dim_devs*5, dim_devs*3], activation_class = otaf.torch.nn.SELU,\n",
    "                                         dropout_class = otaf.torch.nn.AlphaDropout, dropout_kwargs = {'p':0.069}),\n",
    "    *otaf.surrogate.get_custom_mlp_layers([dim_devs*3, dim_devs, 2], activation_class = otaf.torch.nn.Sigmoid),)\n",
    "\n",
    "binary_slack_classifier.optimizer = otaf.torch.optim.AdamW(binary_slack_classifier.parameters(), lr=lr, weight_decay=0.01)  # Lion(binary_slack_classifier.parameters(), lr=lr, weight_decay=0.0001) #Adam(neural_model.model.parameters(), lr=lr)#\n",
    "weight =  otaf.torch.tensor([1.0/(train_success_ratio+1e-16), 1.0/(train_failure_ratio+1e-16)]).float()\n",
    "binary_slack_classifier.criterion = otaf.torch.nn.CrossEntropyLoss(weight=weight, reduction='mean', label_smoothing=0.001) # otaf.torch.nn.BCEWithLogitsLoss(pos_weight=weight) # otaf.torch.nn.MSELoss()\n",
    "otaf.surrogate.initialize_model_weights(binary_slack_classifier)\n",
    "binary_slack_classifier.scheduler = otaf.torch.optim.lr_scheduler.ExponentialLR(binary_slack_classifier.optimizer, 1.0005) #LinearLR(neural_model.optimizer, 1, 0.1, 200)\n",
    "\n",
    "if os.path.exists(save_path) and load:\n",
    "    binary_slack_classifier.load_model()\n",
    "else :\n",
    "    binary_slack_classifier.train_model()\n",
    "    binary_slack_classifier.plot_results(save_as_png=True, save_path='STORAGE/images')\n",
    "    binary_slack_classifier.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85b91257-eaf8-4b15-b534-e982bf189355",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This was for testing, we pass it now \n",
    "if False : \n",
    "    pred_class = binary_slack_classifier.evaluate_model(Xvalid).cpu().detach().numpy()\n",
    "    ground_truth = yvalid[:,-1]#np.array([res.x[-1] for res in ground_truth_full],dtype=\"float32\")\n",
    "    ground_truth_binary = np.where(ground_truth<0,1,0) #ground truth for the failures.\n",
    "    \n",
    "    # Optimize thresholds\n",
    "    optimization_results_fn_tn = otaf.surrogate.optimize_thresholds_with_alpha(pred_class, ground_truth_binary, bounds=[-5.0, 5.0], optimize_for=\"minimize_fn_maximize_tn\", optimal_ratio=1e-3)\n",
    "    optimization_results_tp_fp = otaf.surrogate.optimize_thresholds_with_alpha(pred_class, ground_truth_binary, bounds=[-5.0, 5.0], optimize_for=\"minimize_fp_maximize_tp\", equality_decision=\"success\", optimal_ratio=4*1e-2)\n",
    "    \n",
    "    print(f\"\\nBest Failure Threshold (minimize_fn_maximize_tn): {optimization_results_fn_tn['best_failure_threshold']}\")\n",
    "    print(f\"Best Success Threshold (minimize_fn_maximize_tn): {optimization_results_fn_tn['best_success_threshold']}\")\n",
    "    print(\"Evaluation Metrics (minimize_fn_maximize_tn):\", optimization_results_fn_tn['evaluation'])\n",
    "    print('\\n')\n",
    "    print(f\"Best Failure Threshold (maximize_tp_minimize_fp): {optimization_results_tp_fp['best_failure_threshold']}\")\n",
    "    print(f\"Best Success Threshold (maximize_tp_minimize_fp): {optimization_results_tp_fp['best_success_threshold']}\")\n",
    "    print(\"Evaluation Metrics (maximize_tp_minimize_fp):\", optimization_results_tp_fp['evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a261959-68c2-42e5-a8d5-0dd3d000ea77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using basin hopping with cobyla to optimize thresholds for minimizing classification errors.\n",
      "Using basin hopping with cobyla to optimize thresholds for minimizing classification errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SocAssemblyAnalysisOptimized(\n",
       "  Binary Classifier: BinaryClassificationModel\n",
       "  Constraint Matrix Generator: SystemOfConstraintsAssemblyModel\n",
       "  X Optimization Shape: (250000, 32)\n",
       "  Y Optimization Shape: (250000, 36)\n",
       "  Optimize Results (FN/TN):\n",
       "    Best Failure Threshold: -2.1199193798865923\n",
       "    Best Success Threshold: 1.3054405048795277\n",
       "    Confusion Matrix: \n",
       "      TN: 11090\n",
       "      FP: 115693\n",
       "      FN: 2\n",
       "      TP: 123215\n",
       "  Optimize Results (FP/TP):\n",
       "    Best Failure Threshold: 1.7720199692532086\n",
       "    Best Success Threshold: -2.129186472274088\n",
       "    Confusion Matrix: \n",
       "      TN: 126742\n",
       "      FP: 41\n",
       "      FN: 100898\n",
       "      TP: 22319\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer_milp_ai  = otaf.uncertainty.SocAssemblyAnalysisOptimized(binary_slack_classifier, SOCAM, Xvalid, yvalid)\n",
    "optimizer_milp_ai.optimize_thresholds(bounds=[-5.0, 5.0]) \n",
    "optimizer_milp_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea15a9a-4ad6-4d30-be50-ec7d23d9f275",
   "metadata": {},
   "source": [
    "# Trying to enrich the training points to augment precision, but does not yet work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b12f3aee-04a5-4127-99a8-12a2e544ba6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pass_data_aug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3480b312-154c-4fe3-acb5-d8509cbef7bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not pass_data_aug:\n",
    "    bounds = None\n",
    "    SEED_MC_PF = 6436431\n",
    "    SIZE_MC_PF = int(2*1e5) #1e4\n",
    "    failure_probabilities, new_training_samples = [], []\n",
    "    \n",
    "    ot.RandomGenerator.SetSeed(SEED_MC_PF)\n",
    "    sample = np.array(RandDeviationVect.getSample(SIZE_MC_PF))\n",
    "    print('Generated Sample.')\n",
    "    \n",
    "    start_time = time()  # Record the start time\n",
    "    for i in range(N_lambda):        \n",
    "    \n",
    "        sample_lambd = sample * np.array(lambda_sample_conditioned[i])\n",
    "        \n",
    "        failures, smp_res = optimizer_milp_ai.soc_optimization_sample(sample_lambd, n_cpu=-2, batch_size=500, progress_bar=False, batch_size_ai=int(1e6))\n",
    "        failure_prob = failures.mean()\n",
    "        failure_probabilities.append(failure_prob)\n",
    "        new_training_samples.append(smp_res)\n",
    "        print(f\"--> Iteration {i + 1:03d}. Failures: {int(failures.sum()):04d}, Probability : {failure_prob:.3E}. Min: {min(failure_probabilities):.3E}, / Max: {max(failure_probabilities):.3E}\")\n",
    "        \n",
    "    print(f\"Done {len (lambda_sample_conditioned)} experiments.\")\n",
    "    print(f\"Elapsed time: {time() - start_time:.3f} seconds.\")\n",
    "    \n",
    "    X = otaf.uncertainty.find_best_worst_quantile(np.array(lambda_sample_conditioned), np.array(failure_probabilities), 0.1)\n",
    "    (best_5p_lambda, best_5p_res), (worst_5p_lambda, worst_5p_res) = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed5bb66e-afb0-4275-8e60-80676d63652a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if not pass_data_aug:\n",
    "    # Initialize lists to hold the individual matrices\n",
    "    a_list = []\n",
    "    b_list = []\n",
    "    \n",
    "    # Iterate through the list of tuples and separate them into two lists\n",
    "    for a, b in new_training_samples:\n",
    "        a_list.append(a)\n",
    "        b_list.append(b)\n",
    "    \n",
    "    # Convert the lists to numpy arrays\n",
    "    a_array = np.concatenate(a_list)\n",
    "    b_array = np.concatenate(b_list)\n",
    "    \n",
    "    # Verify the shapes\n",
    "    print(f\"Shape of a_array: {a_array.shape}\")  # Expected shape (2, 3, 4)\n",
    "    print(f\"Shape of b_array: {b_array.shape}\")  # Expected shape (2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09cbf61c-b5c3-45d1-9e2a-7c83e8419c4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not pass_data_aug:\n",
    "    binary_slack_classifier.add_new_data_points(a_array, b_array)\n",
    "    binary_slack_classifier.train_model()\n",
    "    binary_slack_classifier.plot_results()\n",
    "    best_worst_quants = otaf.uncertainty.find_best_worst_quantile(np.array(lambda_sample_conditioned)**2, np.array(failure_probabilities), 0.2)\n",
    "    (best_5p_lambda, best_5p_res), (worst_5p_lambda, worst_5p_res) = best_worst_quants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5510cc7c-8b9a-48ba-83f6-a2ec9dedebef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if not pass_data_aug:\n",
    "    print(\"Lower probability of failure:\", \"{:.5e}\".format(min(failure_probabilities)))\n",
    "    print(\"Upper probability of failure:\", \"{:.5e}\".format(max(failure_probabilities)))\n",
    "    plt.hist(failure_probabilities)\n",
    "    plt.show()\n",
    "    otaf.plotting.plot_best_worst_results(best_5p_res, worst_5p_res, figsize=(10,5))\n",
    "    \n",
    "    variable_labels = [var for var in lambda_sample_conditioned.getDescription()]\n",
    "    otaf.plotting.plot_best_worst_input_data(best_5p_lambda[:3], worst_5p_lambda[:3], variable_labels, figsize=(20,5), labels=False)\n",
    "    #Upper probability of failure: 5.64000e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66991a86-3b21-4fb1-a190-6f78fb8dc8df",
   "metadata": {},
   "source": [
    "# Using the score function aproach on the neural surrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a14f8ec0-3213-4c10-a330-9d5e28341a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold and scaling factors\n",
    "scale_factor = 1.0  # Adjust this scaling factor for your specific range\n",
    "SEED = 38421668465243\n",
    "\n",
    "N_SAMPLE_MINI = int(2*1e5)\n",
    "N_SAMPLE_GLD = 2*int(1e4)\n",
    "standards = np.array([RandDeviationVect.getParameter()[i] for i , param in enumerate(RandDeviationVect.getParameterDescription()) if \"sigma\" in param]) \n",
    "means = np.array([RandDeviationVect.getParameter()[i] for i , param in enumerate(RandDeviationVect.getParameterDescription()) if \"mu\" in param])\n",
    "ot.RandomGenerator.SetSeed(SEED)\n",
    "sample = otaf.sampling.generate_and_transform_sequence(RandDeviationVect.getDimension(), N_SAMPLE_MINI, RandDeviationVect, sequence_type='halton')*1.5\n",
    "sample_gld = otaf.sampling.generate_and_transform_sequence(RandDeviationVect.getDimension(), N_SAMPLE_GLD, RandDeviationVect, sequence_type='halton')\n",
    "#sample = np.array(RandDeviationVect.getSample(N_SAMPLE_MINI))*1.5\n",
    "threshold = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a16f04df-c2b8-4767-8bae-e49b3ab8ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    res = optimizer_milp_ai.soc_optimization_sample(x, n_cpu=-2, batch_size=1000, progress_bar=False, batch_size_ai=int(1e6))\n",
    "    return res[0]\n",
    "\n",
    "def model_base(x, sample=sample_gld):\n",
    "    # Model without surrogate, to get slack\n",
    "    x = sample * np.sqrt(x[np.newaxis, :])\n",
    "    lp_optimization_results = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "        constraint_matrix_generator=SOCAM,\n",
    "        deviation_array=x,\n",
    "        batch_size=5000,\n",
    "        n_cpu=4,\n",
    "        progress_bar=True,\n",
    "        verbose=0,\n",
    "        dtype=\"float32\",\n",
    "    )\n",
    "\n",
    "    slack_values = np.array([x[-1] for x in lp_optimization_results])\n",
    "    return slack_values\n",
    "\n",
    "@otaf.optimization.scaling(scale_factor)\n",
    "def optimization_function_mini(x, getJac=True, model=model): \n",
    "    if getJac:\n",
    "        res = otaf.uncertainty.monte_carlo_non_compliancy_rate_w_gradient(\n",
    "            threshold, sample, means, standards, model, model_is_bool=True)(x)\n",
    "        return res[0], res[1]\n",
    "    else:\n",
    "        x = sample * np.sqrt(x[np.newaxis, :])\n",
    "\n",
    "        return model(x).mean()  # Scale mean value\n",
    "\n",
    "@otaf.optimization.scaling(scale_factor)\n",
    "def optimization_function_maxi(x, getJac=True, model=model): \n",
    "    if getJac:\n",
    "        res = otaf.uncertainty.monte_carlo_non_compliancy_rate_w_gradient(\n",
    "            threshold, sample, means, standards, model, model_is_bool=True)(x)\n",
    "        return -1 * res[0], -1 * res[1]\n",
    "    else:\n",
    "        x = sample * np.sqrt(x[np.newaxis, :])\n",
    "        return -1 * model(x).mean()  # Scale mean value\n",
    "\n",
    "\n",
    "# Define the callback function\n",
    "def print_callback(xk):\n",
    "    print(f\"Current parameter values: {xk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c15ca56c-0c6f-458e-9941-7d1f184f7948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52862135b4814c8481228c6853dca729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slack = model_base(np.array([0.0, 0.0, 0.0, 1.0]*8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2b054b5-3ec5-462e-ae38-578657025664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being less than 0: [0.0098031]\n"
     ]
    }
   ],
   "source": [
    "#_ = plt.hist(slack, bins=100)\n",
    "gld = GLD('VSL')\n",
    "param_LMM = gld.fit_LMM(slack, disp_fit=False, disp_optimizer=False)\n",
    "# Assuming 'param' contains the fitted parameters and gld is an instance of the GLD class\n",
    "probability = gld.CDF_num(0, param_LMM)\n",
    "print(f\"Probability of being less than 0: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a8e402-a8e1-435f-8928-35b9a4aa1ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63e7888b-fb26-477f-9e87-0be88c055dd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qergqrg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m qergqrg\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qergqrg' is not defined"
     ]
    }
   ],
   "source": [
    "qergqrg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3455c-e718-425a-91f3-fa6668c957c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b72bd-8906-47ef-81b7-86088bd1e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(slack<0,1,0).sum()/20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a4ea3-591e-4cb8-9c11-639b18e7ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "def solve_for_lambda_beta(lambda_u, lambda_v, lambda_alpha):\n",
    "    \"\"\"\n",
    "    Solve the equation for lambda_beta given lambda_u, lambda_v, and lambda_alpha.\n",
    "    \"\"\"\n",
    "    def equation(lambda_beta):\n",
    "        return (lambda_u**2 + lambda_v**2 + lambda_alpha**2 + lambda_beta**2 +\n",
    "                2 * (lambda_u * lambda_alpha + lambda_v * lambda_beta) - 1)\n",
    "    \n",
    "    # Use a root-finding method to solve for lambda_beta\n",
    "    lambda_beta_initial_guess = 0.5\n",
    "    lambda_beta_solution = fsolve(equation, lambda_beta_initial_guess)[0]\n",
    "    \n",
    "    return lambda_beta_solution\n",
    "\n",
    "def generate_points_on_surface(num_points=1000):\n",
    "    \"\"\"\n",
    "    Generate points on the surface by solving for lambda_beta\n",
    "    given random values for lambda_u, lambda_v, and lambda_alpha.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of shape (num_valid_points, 4) where each row is \n",
    "      (lambda_u, lambda_v, lambda_alpha, lambda_beta).\n",
    "    \"\"\"\n",
    "    # Generate random values for lambda_u, lambda_v, lambda_alpha in [0, 1]\n",
    "    lambda_u = np.random.rand(num_points)\n",
    "    lambda_v = np.random.rand(num_points)\n",
    "    lambda_alpha = np.random.rand(num_points)\n",
    "    \n",
    "    # Assume lambda_beta = 0 and check if the inequality is satisfied\n",
    "    inequality_values = lambda_u**2 + lambda_v**2 + lambda_alpha**2 + 2 * (lambda_u * lambda_alpha)\n",
    "\n",
    "    # Filter valid points where inequality holds\n",
    "    valid_mask = inequality_values <= 1\n",
    "    lambda_u_valid = lambda_u[valid_mask]\n",
    "    lambda_v_valid = lambda_v[valid_mask]\n",
    "    lambda_alpha_valid = lambda_alpha[valid_mask]\n",
    "    \n",
    "    # Solve for lambda_beta for valid points\n",
    "    lambda_beta_valid = np.array([solve_for_lambda_beta(u, v, a) \n",
    "                                  for u, v, a in zip(lambda_u_valid, lambda_v_valid, lambda_alpha_valid)])\n",
    "\n",
    "    # Only keep points where the solved lambda_beta is between 0 and 1\n",
    "    final_mask = (lambda_beta_valid >= 0) & (lambda_beta_valid <= 1)\n",
    "    final_points = np.vstack((lambda_u_valid[final_mask],\n",
    "                              lambda_v_valid[final_mask],\n",
    "                              lambda_alpha_valid[final_mask],\n",
    "                              lambda_beta_valid[final_mask])).T\n",
    "    \n",
    "    return final_points\n",
    "\n",
    "# Generate the points on the surface\n",
    "points = generate_points_on_surface(num_points=100000)\n",
    "\n",
    "# Call the pair_plot function (assuming it's already defined)\n",
    "labels = ['lambda_u', 'lambda_v', 'lambda_alpha', 'lambda_beta']\n",
    "otaf.plotting.pair_plot(points, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe8fd0-e8c7-4824-afc4-101d7ff0fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrgw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169cda6-f6a2-448c-9a20-0f2bcf989c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons, linearConstraint = otaf.optimization.lambda_constraint_dict_from_composed_distribution(RandDeviationVect, tol=0, keep_feasible=False)\n",
    "bounds_lambda = otaf.optimization.bounds_from_composed_distribution(RandDeviationVect, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85844dc-9afa-4fcc-a94d-b86b23db0531",
   "metadata": {},
   "source": [
    "### Some checks to compare difference and score based gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee48726-49a6-4811-842d-b5ecfd34e654",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "x0_maxi = np.array([0.25]*RandDeviationVect.getDimension()) # [0.1,0.1,0.4,0.4]*8  # Initial guess\n",
    "for i in range(len(x0_maxi)):\n",
    "    x_perturb = x0_maxi.copy()\n",
    "    x_perturb[i] += 0.05\n",
    "    f_val = optimization_function_maxi(x_perturb, getJac=False)\n",
    "    print(f\"Perturbing parameter {i}: Objective value: {f_val}\")\n",
    "\n",
    "_, jac_score = optimization_function_maxi(x0_maxi, getJac=True)\n",
    "\n",
    "jac_approx = approx_fprime(x0_maxi, optimization_function_maxi, 0.1, False)\n",
    "otaf.plotting.compare_jacobians(jac_score, jac_approx, class_labels=otaf.sampling.validate_and_extract_indices(SOCAM.deviation_symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2132bb-d3f1-4a00-868b-50cde0dd4ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x0_maxi = [0.25]*RandDeviationVect.getDimension() # [0.1,0.1,0.4,0.4]*8  # Initial guess\n",
    "\n",
    "res = minimize(optimization_function_maxi, x0_maxi, \n",
    "               method=\"COBYQA\", jac=False, args=(False,), \n",
    "               bounds=bounds_lambda, \n",
    "               constraints=linearConstraint, \n",
    "               options={\"f_target\":-0.2, \n",
    "                        \"maxiter\":50,\n",
    "                        \"maxfev\":150,\n",
    "                        \"feasibility_tol\":1e-4,\n",
    "                        \"initial_tr_radius\":1,\n",
    "                        \"final_tr_radius\":1e-3,\n",
    "                       \"disp\":True}, \n",
    "               callback = print_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8596b81-7d32-49ed-99d1-99a70ebe45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab91c6-d998-41d1-8739-502140b07ef4",
   "metadata": {},
   "source": [
    "# Global optimization basinhopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a3a13-3088-4075-8f2f-5ea080fd0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint_checker = otaf.optimization.create_constraint_checker(linearConstraint,0)\n",
    "opt_storage = otaf.optimization.OptimizationStorage(bounds_lambda, constraint_checker)\n",
    "step_taking = otaf.optimization.StepTaking(opt_storage, SOCAM.deviation_symbols)\n",
    "accept_test = otaf.optimization.AcceptTest(opt_storage, SOCAM.deviation_symbols)\n",
    "callback = otaf.optimization.Callback(opt_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb5870-0875-4677-9efe-c2bee9e9f7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basinhopping for the maximization function using COBYQA\n",
    "x0_maxi = [0.25] * RandDeviationVect.getDimension()  # Initial guess\n",
    "\n",
    "# Update minimizer_kwargs_maxi to use COBYQA\n",
    "minimizer_kwargs_maxi = {\n",
    "    \"method\": \"COBYQA\",   # Use COBYQA method\n",
    "    \"jac\": False,         # COBYQA doesn't use Jacobians\n",
    "    \"args\": (False,),     # Update args to match COBYQA requirements\n",
    "    \"constraints\": linearConstraint,\n",
    "    \"bounds\": bounds_lambda,\n",
    "    \"options\": {\n",
    "        \"f_target\": -0.2, \n",
    "        \"maxiter\": 50,\n",
    "        \"maxfev\": 150,\n",
    "        \"feasibility_tol\": 1e-4,\n",
    "        \"initial_tr_radius\": 1,\n",
    "        \"final_tr_radius\": 1e-3,\n",
    "        \"disp\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Running basinhopping with COBYQA as the local optimizer\n",
    "res_maxi = basinhopping(\n",
    "    optimization_function_maxi, x0_maxi,\n",
    "    niter=80,\n",
    "    T=1,\n",
    "    stepsize=3.0,\n",
    "    niter_success=19,\n",
    "    interval=5,\n",
    "    minimizer_kwargs=minimizer_kwargs_maxi,\n",
    "    disp=True,\n",
    "    take_step=step_taking,\n",
    "    accept_test=accept_test,\n",
    "    callback=callback\n",
    ")\n",
    "\n",
    "print(\"Maximization Result with COBYQA:\")\n",
    "print(res_maxi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3c1a4-7d8f-441d-a59d-6be0897b7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_maxi.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89e4956-1786-4769-b3bc-57497860698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_function_maxi(np.array([0,0,0.5,0.5]*8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d956b7-96c8-4c2a-9967-c5b156c6f538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basinhopping for the maximization function\n",
    "x0_maxi = [0.25]*RandDeviationVect.getDimension() # Initial guess\n",
    "\n",
    "minimizer_kwargs_maxi = {\n",
    "    \"method\": \"SLSQP\",\n",
    "    \"jac\":True,\n",
    "    \"args\": (True, model),\n",
    "    \"constraints\": linearConstraint,\n",
    "    \"bounds\": bounds_lambda,\n",
    "    \"options\": {\"disp\": True, \"maxiter\": 500, \"ftol\": 1e-4}\n",
    "}\n",
    "\n",
    "# Trying custom basin  hopping\n",
    "res_maxi = basinhopping(optimization_function_maxi, x0_maxi, \n",
    "                        niter=80, \n",
    "                        T=1, \n",
    "                        stepsize=3.0, #2.3, \n",
    "                        niter_success=19,\n",
    "                        interval=5,\n",
    "                        minimizer_kwargs=minimizer_kwargs_maxi, \n",
    "                        disp=True,\n",
    "                        take_step=step_taking,\n",
    "                        accept_test=accept_test,\n",
    "                        callback=callback)\n",
    "\n",
    "print(\"Maximization Result:\")\n",
    "print(res_maxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337a0dd-b891-4df3-adac-641e71ee0cae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basinhopping for the minimization function\n",
    "x0_mini = [0.25]*RandDeviationVect.getDimension()   # Initial guess\n",
    "\n",
    "minimizer_kwargs_mini = {\n",
    "    \"method\": \"SLSQP\",\n",
    "    \"args\": (True),\n",
    "    \"constraints\": cons,\n",
    "    \"bounds\": bounds_lambda,\n",
    "    \"options\": {\"disp\": True, \"maxiter\": 100, \"ftol\": 1e-6, \"eps\":0.1},\n",
    "    \"jac\":True\n",
    "}\n",
    "\n",
    "res_mini = basinhopping(optimization_function_mini, x0_mini,\n",
    "                        niter=80, \n",
    "                        T=0.1, \n",
    "                        stepsize=1.3, \n",
    "                        niter_success=19,\n",
    "                        interval=5,\n",
    "                        target_accept_rate=0.69,\n",
    "                        stepwise_factor=0.69,                        \n",
    "                        minimizer_kwargs=minimizer_kwargs_maxi, disp=True)\n",
    "\n",
    "print(\"Minimization Result:\")\n",
    "print(res_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722dbae4-3353-4e60-b4a2-87d03e6d7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_maxi.lowest_optimization_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade564f7-369b-4d12-8d4e-72f52f1e39d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2721d037-77fb-4402-93c0-19e6c086e7fb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def custom_format(value):\n",
    "    \"\"\"\n",
    "    Custom format function to format numbers in scientific notation,\n",
    "    but remove the 'E+00' if the exponent is zero and remove leading zero in the exponent.\n",
    "    \"\"\"\n",
    "    formatted = f\"{value:.1E}\"\n",
    "    if \"E+00\" in formatted:\n",
    "        return formatted.replace(\"E+00\", \"\")\n",
    "    else:\n",
    "        # Remove leading zeros in the exponent\n",
    "        formatted = re.sub(r\"E([+-])0*(\\d+)\", r\"E\\1\\2\", formatted)\n",
    "        return formatted\n",
    "\n",
    "def plot_bar(values, labels, ylim=[0,1],\n",
    "            title = 'Relative contribution of each DOF to the total variability of the feature',\n",
    "            ylabel = 'Contribution'):\n",
    "    \"\"\"\n",
    "    Plots a bar plot with given values and LaTeX formatted labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - values: List or array of floats, values to plot.\n",
    "    - labels: List of LaTeX strings, corresponding labels for the values.\n",
    "    \"\"\"\n",
    "    # Generate the LaTeX formatted labels\n",
    "    variable_labels_tex = [f\"${sp.printing.latex(sp.Symbol(var))}$\" for var in labels]\n",
    "\n",
    "    # Define a color mapping dictionary for variables with the same index\n",
    "    color_mapping = {}\n",
    "    color_idx = 0\n",
    "    for var_label in labels:\n",
    "        index = re.match(\".*?([0-9]+)$\", str(var_label)).group(\n",
    "            1\n",
    "        )  # Extract the index part (e.g., '0')\n",
    "        if index not in color_mapping:\n",
    "            color_idx += 1\n",
    "            color_mapping[index] = otaf.plotting.hex_to_rgba(otaf.plotting.color_palette_3[color_idx], as_float=True)\n",
    "    \n",
    "    # Create the bar plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for i, var_label in enumerate(labels):\n",
    "        var_index = int(re.match(\".*?([0-9]+)$\", str(var_label)).group(1))\n",
    "\n",
    "        ax.axvspan(i - 0.5, i + 0.5, facecolor=color_mapping[str(var_index)], alpha=0.3)\n",
    "        \n",
    "    bars = ax.bar(range(len(values)), values, tick_label=variable_labels_tex, align='center')\n",
    "    print('Hey', bars)\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel('Degrees of freedom', fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    di = ylim[1] - ylim[0]\n",
    "    upper_count = 0\n",
    "    lower_count = 0\n",
    "    \n",
    "    # Add text box for values out of y-axis scope\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        if height > ylim[1]:\n",
    "            upper_count += 1\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, ylim[1] - (0.025 * di) - (0.05 * di * upper_count), custom_format(height), ha='center', va='bottom', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
    "        else:\n",
    "            upper_count = 0\n",
    "        \n",
    "        if height < ylim[0]:\n",
    "            lower_count += 1\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, ylim[0] + (0.025 * di) + (0.05 * di * lower_count), custom_format(height), ha='center', va='top', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))\n",
    "        else:\n",
    "            lower_count = 0\n",
    "    \n",
    "    # Rotate the x labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c949be-a00e-4560-8934-53c8ddf41b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_smp = [0.01,0.01,0.97,0.01]*8#best_5p_lambda[2]\n",
    "res2 = optimization_function_maxi(best_smp)\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca909f-5469-43ac-9d45-dd5995489ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "plot_bar(best_smp, RandDeviationVect.getDescription(), [0,1.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd58a80-2b4d-446a-8c51-833a82d59f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar(res2[1], RandDeviationVect.getDescription(), [-1, 1], 'Score function for each degree of freedom', \"Score function value x 100\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
