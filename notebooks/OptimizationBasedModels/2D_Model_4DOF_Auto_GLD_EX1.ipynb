{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ecb329a-8614-42a5-82a8-78a3d5dacada",
   "metadata": {},
   "source": [
    "# Tolerance analysis of simple 1.5D model, automatic dictionary construction, crude monte carlo, optimizaiton on epistemic space of uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd254d60-fd5b-4c8e-9f4a-1de5de524e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import pprint\n",
    "import numpy as np\n",
    "import sympy as sp\n",
    "import scipy\n",
    "import openturns as ot\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh as tr\n",
    "\n",
    "from math import pi, sqrt\n",
    "from joblib import Parallel, delayed\n",
    "from importlib import reload\n",
    "from IPython.display import display, clear_output, HTML, IFrame\n",
    "from time import time, sleep\n",
    "from sympy.printing import latex\n",
    "from trimesh import viewer as trview\n",
    "from scipy.optimize import OptimizeResult, minimize, basinhopping, \\\n",
    "                           differential_evolution, brute, shgo, check_grad, \\\n",
    "                           approx_fprime, fsolve, NonlinearConstraint, Bounds, approx_fprime\n",
    "import torch\n",
    "import otaf\n",
    "\n",
    "from gldpy import GLD\n",
    "\n",
    "ot.Log.Show(ot.Log.NONE)\n",
    "np.set_printoptions(suppress=True)\n",
    "ar = np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8275252e-374d-44e2-b204-8764502e12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different measures of our problem\n",
    "X1 = 99.8   # Nominal Length of the male piece\n",
    "X2 = 100.0  # Nominal Length of the female piece\n",
    "X3 = 10.0   # Nominal height of the pieces\n",
    "t = 0.2*sqrt(2)    # Tolerance for X1 and X2. (95% conform)  (= t/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d5ac3-79f9-47d1-8329-ba5dce619632",
   "metadata": {},
   "source": [
    "## Coordinates, points, feature definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c637ce58-ba04-4d7d-a03f-441b1d8e60d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global coordinate system\n",
    "R0 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n",
    "x_, y_, z_ = R0[0], R0[1], R0[2]\n",
    "\n",
    "# Important points\n",
    "# Pièce 1 (male)\n",
    "P1A0, P1A1, P1A2 = (\n",
    "    np.array((0, X3 / 2, 0.0)),\n",
    "    np.array((0, X3, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    ")\n",
    "P1B0, P1B1, P1B2 = (\n",
    "    np.array((X1, X3 / 2, 0.0)),\n",
    "    np.array((X1, X3, 0.0)),\n",
    "    np.array((X1, 0, 0.0)),\n",
    ")\n",
    "P1C0, P1C1, P1C2 = (\n",
    "    np.array((X1 / 2, 0, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    "    np.array((X1, 0, 0.0)),\n",
    ")\n",
    "\n",
    "# Pièce 2 (femelle)  # On met les points à hM et pas hF pour qu'ils soient bien opposées! (Besoin??)\n",
    "P2A0, P2A1, P2A2 = (\n",
    "    np.array((0, X3 / 2, 0.0)),\n",
    "    np.array((0, X3, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    ")\n",
    "P2B0, P2B1, P2B2 = (\n",
    "    np.array((X2, X3 / 2, 0.0)),\n",
    "    np.array((X2, X3, 0.0)),\n",
    "    np.array((X2, 0, 0.0)),\n",
    ")\n",
    "P2C0, P2C1, P2C2 = (\n",
    "    np.array((X2 / 2, 0, 0.0)),\n",
    "    np.array((0, 0, 0.0)),\n",
    "    np.array((X2, 0, 0.0)),\n",
    ")\n",
    "\n",
    "# Local coordinate systems\n",
    "# Pièce1\n",
    "RP1a = np.array([-1 * x_, -1 * y_, z_])\n",
    "RP1b = R0\n",
    "RP1c = np.array([-y_, x_, z_])\n",
    "\n",
    "# Pièce2\n",
    "RP2a = R0\n",
    "RP2b = np.array([-1 * x_, -1 * y_, z_])\n",
    "RP2c = np.array([y_, -1 * x_, z_])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ed640e-40fc-48fe-ab9a-b46b5fd97b61",
   "metadata": {},
   "source": [
    "### Construction of the augmented system data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1248a3-5053-4030-b165-c48863d8d2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_data = {\n",
    "    \"PARTS\" : {\n",
    "        '1' : {\n",
    "            \"a\" : {\n",
    "                \"FRAME\": RP1a,\n",
    "                \"POINTS\": {'A0' : P1A0, 'A1' : P1A1, 'A2' : P1A2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P2a'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"], # In this modelization, only defects on the right side\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"b\" : {\n",
    "                \"FRAME\": RP1b,\n",
    "                \"POINTS\": {'B0' : P1B0, 'B1' : P1B1, 'B2' : P1B2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P2b'],\n",
    "                \"CONSTRAINTS_D\": [\"NONE\"],\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"c\" : {\n",
    "                \"FRAME\": RP1c,\n",
    "                \"POINTS\": {'C0' : P1C0, 'C1' : P1C1, 'C2' : P1C2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P2c'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"],\n",
    "                \"CONSTRAINTS_G\": [\"SLIDING\"],            \n",
    "            },\n",
    "        },\n",
    "        '2' : {\n",
    "            \"a\" : {\n",
    "                \"FRAME\": RP2a,\n",
    "                \"POINTS\": {'A0' : P2A0, 'A1' : P2A1, 'A2' : P2A2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P1a'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"], # In this modelization, only defects on the right side\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"b\" : {\n",
    "                \"FRAME\": RP2b,\n",
    "                \"POINTS\": {'B0' : P2B0, 'B1' : P2B1, 'B2' : P2B2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P1b'],\n",
    "                \"CONSTRAINTS_D\": [\"NONE\"],\n",
    "                \"CONSTRAINTS_G\": [\"FLOATING\"],            \n",
    "            },\n",
    "            \"c\" : {\n",
    "                \"FRAME\": RP2c,\n",
    "                \"POINTS\": {'C0' : P2C0, 'C1' : P2C1, 'C2' : P2C2},\n",
    "                \"TYPE\": \"plane\",\n",
    "                \"INTERACTIONS\": ['P1c'],\n",
    "                \"CONSTRAINTS_D\": [\"PERFECT\"],\n",
    "                \"CONSTRAINTS_G\": [\"SLIDING\"],            \n",
    "            },\n",
    "        }  \n",
    "    },\n",
    "    \"LOOPS\": {\n",
    "        \"COMPATIBILITY\": {\n",
    "            \"L0\": \"P1cC0 -> P2cC0 -> P2aA0 -> P1aA0\",\n",
    "            \"L1\": \"P1cC0 -> P2cC0 -> P2bB0 -> P1bB0\",\n",
    "        },\n",
    "    },\n",
    "    \"GLOBAL_CONSTRAINTS\": \"2D_NZ\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4b960-047d-47fa-a3ae-0a131bef54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SDA = otaf.AssemblyDataProcessor(system_data)\n",
    "SDA.generate_expanded_loops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d0b7ba-4d97-410f-8ebc-84eacfc537b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLH = otaf.CompatibilityLoopHandling(SDA)\n",
    "compatibility_expressions = CLH.get_compatibility_expression_from_FO_matrices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e9b9d-0911-47ac-b5d2-156b71cd9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ILH = otaf.InterfaceLoopHandling(SDA, CLH, circle_resolution=20)\n",
    "interface_constraints = ILH.get_interface_loop_expressions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ab7b1-7614-43b7-b413-d0f60c7eec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOCAM = otaf.SystemOfConstraintsAssemblyModel(\n",
    "    compatibility_expressions, interface_constraints\n",
    ")\n",
    "\n",
    "SOCAM.embedOptimizationVariable()\n",
    "\n",
    "print(len(SOCAM.deviation_symbols), SOCAM.deviation_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18adf2-2501-4ab0-a3d4-d02d6ff30ed5",
   "metadata": {},
   "source": [
    "## Construction of the stochastic model of the defects. (old lambda approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296e3160-d1fe-4f84-ab1b-804aac69c5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cm = 1.0\n",
    "sigma_e_pos = t / (6 * Cm)\n",
    "\n",
    "# Le défaut en orientation est piloté par une incertitude sur un angle. On suppose les angles petits << 1 rad\n",
    "theta_max = t / X3\n",
    "sigma_e_theta = (2*theta_max) / (6*Cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba9590-7460-496d-8c15-f1979d39e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandDeviationVect = otaf.distribution.get_composed_normal_defect_distribution(\n",
    "    defect_names=SOCAM.deviation_symbols,\n",
    "    sigma_dict = {\"alpha\":sigma_e_theta, \n",
    "                  \"beta\":sigma_e_theta,\n",
    "                  \"gamma\":sigma_e_theta, \n",
    "                  \"u\":sigma_e_pos, \n",
    "                  \"v\":sigma_e_pos, \n",
    "                  \"w\":sigma_e_pos})\n",
    "NDim_Defects = int(RandDeviationVect.getDimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6e791-e715-454a-b51b-27cf1704b1e0",
   "metadata": {},
   "source": [
    "## Construction of a neural network based surrogate \n",
    "(could be omitted but makes things faster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1846e592-66b5-4f84-8220-e6b6818dd844",
   "metadata": {},
   "source": [
    "#### First generate the training sample :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661bb8f1-85e1-4386-8e2a-a95e5645a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the seed, sample size, and file paths\n",
    "SEED = 420  # Example seed value\n",
    "sample_size = 100000\n",
    "\n",
    "# Ensure reproducibility by setting the seed\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Generate the sample\n",
    "dist = otaf.distribution.multiply_composed_distribution_with_constant(\n",
    "    RandDeviationVect, 1.25) # We now work with low failure probabilities so we increase the dispresion to have more failed parts for the training\n",
    "#TRAIN_SAMPLE = np.array(otaf.uncertainty.generateLHSExperiment(dist, sample_size))\n",
    "TRAIN_SAMPLE = np.array(dist.getSample(sample_size),dtype=\"float32\")\n",
    "# Compute the results\n",
    "TRAIN_RESULTS = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "    SOCAM,\n",
    "    TRAIN_SAMPLE,\n",
    "    bounds=None,\n",
    "    n_cpu=-2,\n",
    "    progress_bar=True,\n",
    "    batch_size=500,\n",
    "    dtype=\"float32\"\n",
    ")\n",
    "\n",
    "# Assign X and y from TRAIN_SAMPLE and TRAIN_RESULTS\n",
    "Xtrain = TRAIN_SAMPLE\n",
    "ytrain = TRAIN_RESULTS\n",
    "print(f\"Ratio of failed simulations in sample : {np.where(ytrain[:,-1]<0,1,0).sum()/sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82716e8f-1d74-4ace-ab6c-7baded11854b",
   "metadata": {},
   "source": [
    "#### Then train the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954e704-8bfd-4f23-96ba-44692ecac705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim = int(RandDeviationVect.getDimension())\n",
    "neural_model = otaf.surrogate.NeuralRegressorNetwork(\n",
    "    dim, 1,\n",
    "    Xtrain, ytrain[:,-1], \n",
    "    clamping=True, \n",
    "    finish_critertion_epoch=5,\n",
    "    loss_finish=1e-6, \n",
    "    metric_finish=0.99999, \n",
    "    max_epochs=500, \n",
    "    batch_size=30000, \n",
    "    compile_model=False, \n",
    "    train_size=0.6, \n",
    "    input_description=RandDeviationVect.getDescription(),\n",
    "    display_progress_disable=False)\n",
    "\n",
    "lr=0.003\n",
    "\n",
    "#neural_model.model = KAN([dim, 8, 4, 1])  #otaf.surrogate.get_base_relu_mlp_model(dim, 1, False)\n",
    "\n",
    "neural_model.model = torch.nn.Sequential(\n",
    "    *otaf.surrogate.get_custom_mlp_layers([dim, 100, 70, 30, 1], activation_class=torch.nn.GELU)\n",
    ")\n",
    "\n",
    "neural_model.optimizer = torch.optim.AdamW(neural_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "otaf.surrogate.initialize_model_weights(neural_model)\n",
    "neural_model.scheduler =  torch.optim.lr_scheduler.ExponentialLR(neural_model.optimizer, 1.0001)\n",
    "neural_model.loss_fn = torch.nn.MSELoss()\n",
    "#neural_model.loss_fn = otaf.uncertainty.LimitSpaceFocusedLoss(0.0001, 2, square=True) # otaf.uncertainty.PositiveLimitSpaceFocusedLoss(0.0001, 2, 4, square=False)\n",
    "\n",
    "\n",
    "neural_model.train_model()\n",
    "neural_model.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69344ac5-4131-45b1-afb5-700e3cee16ee",
   "metadata": {},
   "source": [
    "## Optimization on the imprecise space of defects, to get upper and lower probability of failure given the constraints on the defect parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318bc00-42e7-4b24-a3c5-addfbd847565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to store results\n",
    "\n",
    "result_dict={}\n",
    "\n",
    "def store_results(x, fp_gld, fp_slack, gld_params, experiment_key=None, result_dict=result_dict):\n",
    "    x_key = otaf.common.bidirectional_string_to_array_conversion(x)\n",
    "    x_dict = {\"FP_GLD\": fp_gld, \"FP_SLACK\":fp_slack, \"GLD_PARAMS\": gld_params}\n",
    "    if experiment_key is None:\n",
    "        if x_key in result_dict.keys():\n",
    "            result_dict[x_key].update(x_dict)\n",
    "        else :\n",
    "            result_dict[x_key] = x_dict\n",
    "    else : \n",
    "        if experiment_key not in result_dict:\n",
    "            result_dict[experiment_key] = {}\n",
    "        if x_key in result_dict[experiment_key].keys():\n",
    "            result_dict[experiment_key][x_key].update(x_dict)\n",
    "        else:\n",
    "            result_dict[experiment_key][x_key] = x_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7931fd9-0536-4ab4-81c8-cebb80edc302",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_MC_PF = 250000 #int(1e6) #1e4\n",
    "\n",
    "sample_gld = otaf.sampling.generate_and_transform_sequence(NDim_Defects, SIZE_MC_PF, RandDeviationVect) \n",
    "scale_factor = 1.0\n",
    "\n",
    "# Generalized lambda distribution object for fitting\n",
    "gld = GLD('VSL')\n",
    "\n",
    "def model(x, sample=sample_gld):\n",
    "    # Direct model without ai\n",
    "    x = sample * x[np.newaxis, :]\n",
    "    gap_variable_array = otaf.uncertainty.compute_gap_optimizations_on_sample_batch(\n",
    "        SOCAM, x, n_cpu=-1, progress_bar=True\n",
    "    )\n",
    "    slack_variable = gap_variable_array[:, -1]\n",
    "    return slack_variable\n",
    "\n",
    "def model2(x, sample=sample_gld): \n",
    "    # Surrogate ai model\n",
    "    x = sample * x[np.newaxis, :]\n",
    "    return np.squeeze(neural_model.evaluate_model_non_standard_space(x).detach().numpy())\n",
    "\n",
    "@otaf.optimization.scaling(scale_factor)\n",
    "def optimization_function_mini(x, failure_slack=0.0, model=model2, experiment_key=None, result_dict=result_dict):\n",
    "    # Here we search the minimal probability of failure\n",
    "    slack = model(x)\n",
    "    gld_params = gld.fit_LMM(slack,  disp_fit=False, disp_optimizer=False)\n",
    "    fp_slack = np.where(slack<failure_slack,1,0).mean()\n",
    "    fp_gld = np.nan\n",
    "    if np.any(np.isnan(gld_params)):\n",
    "        fp_out = fp_slack\n",
    "    else :\n",
    "        #print(\"\\tgld_params:\", gld_params)\n",
    "        fp_gld = gld.CDF_num(failure_slack, gld_params)\n",
    "        fp_out = fp_gld\n",
    "    \n",
    "    store_results(x, fp_gld, fp_slack, gld_params, experiment_key, result_dict)\n",
    "    return fp_out\n",
    "\n",
    "\n",
    "@otaf.optimization.scaling(scale_factor)\n",
    "def optimization_function_maxi(x, failure_slack=0.0, model=model2, experiment_key=None, result_dict=result_dict):\n",
    "    # Here we search the maximal probability of failure so negative output\n",
    "    slack = model(x)\n",
    "    gld_params = gld.fit_LMM(slack, disp_fit=False, disp_optimizer=False)\n",
    "    fp_slack = np.where(slack<failure_slack,1,0).mean()\n",
    "    fp_gld = np.nan\n",
    "    if np.any(np.isnan(gld_params)):\n",
    "        fp_out = fp_slack\n",
    "    else :\n",
    "        #print(\"\\tgld_params:\", gld_params)\n",
    "        fp_gld = gld.CDF_num(failure_slack, gld_params)\n",
    "        fp_out = fp_gld\n",
    "    \n",
    "    store_results(x, fp_gld, fp_slack, gld_params, experiment_key, result_dict)\n",
    "\n",
    "    return fp_out*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3dc012-2dfc-4e89-8b95-7c457173587e",
   "metadata": {},
   "source": [
    "# Definition of the FLDPCF constraint function\n",
    "\n",
    "Variance (std) based parameter constraint function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd42d5f-3761-4b53-8c84-8ea6f487aab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "midof_funcs = otaf.tolerances.MiSdofToleranceZones()\n",
    "\n",
    "feature_constraint_list = []\n",
    "\n",
    "# We know that all features are parallel lines, with same values/dimensions\n",
    "for i in range(2):\n",
    "    fconst = otaf.tolerances.FeatureLevelStatisticalConstraint(\n",
    "        midof_funcs.two_parallel_straight_lines,\n",
    "        mif_args = (t, X3),\n",
    "        n_dof = 2,\n",
    "        n_sample = 80000,\n",
    "        target = \"std\", #\"prob\",\n",
    "        target_val = sigma_e_pos*np.sqrt(1-(2/np.pi)), #To be folded normal variance!\n",
    "        isNormal = True, #The component distributions are normal\n",
    "        normalizeOutput = True, #Normalize constraint\n",
    "    )\n",
    "    feature_constraint_list.append(fconst)\n",
    "\n",
    "# The input of this object would be a list of parameters (their real value)\n",
    "composed_assembly_constraint = otaf.tolerances.ComposedAssemblyLevelStatisticalConstraint(feature_constraint_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb600b47-44ad-45fd-93da-f5a29efba012",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_bounds_one_feature = [[0.0,0.0], [0.0, sigma_e_pos], #u, mean std\n",
    "                            [0.0,0.0], [0.0, sigma_e_theta] # alpha, mean std\n",
    "                           ]\n",
    "param_bounds = [param_bounds_one_feature] * 2 #We have 2 identical features wth defects\n",
    "\n",
    "# The input of this object is a list of normalized parameters (between 0 and 1)\n",
    "normalized_assembly_constraint = otaf.tolerances.NormalizedAssemblyLevelConstraint(\n",
    "    composed_assembly_constraint,\n",
    "    param_val_bounds=param_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec57d95-46e9-47b3-b057-b68a31b31d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembly_constraint_no_mean(x, scale_factor=1.0, result_dict=result_dict, experiment_key=None):\n",
    "    \"\"\" The functions takes directly the concatenated list of all normalized parameters (between 0 and 1), \n",
    "    and reconstructs the statistical constraint violation for each feature.\n",
    "    \"\"\"\n",
    "    assert len(x)==4, \"problem with input.\"\n",
    "    zer = np.zeros(2) # These are the mean values (all 0)\n",
    "    x = np.array(x)\n",
    "    params_for_assembly = []\n",
    "    for i in range(2):\n",
    "        params = x[i*2:i*2+2]\n",
    "        pa = [item for pair in zip(zer, params) for item in pair]\n",
    "        params_for_assembly.append(pa)\n",
    "    constraint_array =  normalized_assembly_constraint(params_for_assembly)\n",
    "    \n",
    "    # Storing data\n",
    "    x_key = otaf.common.bidirectional_string_to_array_conversion(x)\n",
    "    data = {\"CONST\":constraint_array}\n",
    "    if experiment_key is not None:\n",
    "        if x_key in result_dict[experiment_key].keys():\n",
    "            result_dict[experiment_key][x_key].update(data)\n",
    "        else:\n",
    "            result_dict[experiment_key][x_key] = data\n",
    "    else :\n",
    "        if x_key in result_dict.keys():\n",
    "            result_dict[x_key].update(data)\n",
    "        else :\n",
    "            result_dict[x_key] = data\n",
    "            \n",
    "    return constraint_array * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbacdf4-8ee7-4ad2-8aa6-b408c566d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the nonlinear constraint with the updated vector-valued function and Jacobian\n",
    "nonLinearConstraint = lambda resDict, expKey : NonlinearConstraint(\n",
    "    fun = lambda x : assembly_constraint_no_mean(x, 1.0, resDict, expKey),\n",
    "    lb  = -0.005 * np.ones((2,)),\n",
    "    ub  = 0.005 * np.ones((2,)),\n",
    "    keep_feasible=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e5c11-de56-4437-99f5-9019839cc18a",
   "metadata": {},
   "source": [
    "# Optimizations to find the paramter values that maximize and minimize the rejection rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63241638-cc40-480a-9692-8e4dc7c9859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pf_min_max_optimizer(failure_slack=0.0, result_dict=result_dict, experiment_key=None):\n",
    "    # Initial guess\n",
    "    x0 = [0.5] * NDim_Defects  # Initial guess\n",
    "    \n",
    "    # Perform the local optimization using COBYQA directly\n",
    "    res_maxi = minimize(\n",
    "        optimization_function_maxi, x0,\n",
    "        args=(failure_slack, model2, experiment_key, result_dict),\n",
    "        method=\"COBYQA\", \n",
    "        jac=None, \n",
    "        bounds=Bounds(0.0, 1.0, keep_feasible=True),\n",
    "        constraints = nonLinearConstraint(result_dict, experiment_key),\n",
    "        options={\n",
    "            \"f_target\": -1.01, \n",
    "            \"maxiter\": 400,\n",
    "            \"maxfev\": 400,\n",
    "            \"feasibility_tol\": 1e-6,\n",
    "            \"initial_tr_radius\": np.sqrt(2*10),\n",
    "            \"final_tr_radius\": 1e-5,\n",
    "            \"disp\": False,\n",
    "            \"scale\": False\n",
    "        }\n",
    "    )\n",
    "    print('Maximization result:\\n', res_maxi)\n",
    "    \n",
    "    # Perform the local optimization using COBYQA directly\n",
    "    res_mini = minimize(\n",
    "        optimization_function_mini, x0, \n",
    "        args=(failure_slack, model2, experiment_key, result_dict),\n",
    "        method=\"COBYQA\", \n",
    "        jac=None, \n",
    "        bounds=Bounds(1e-16, 1.0, keep_feasible=True),\n",
    "        constraints = nonLinearConstraint(result_dict, experiment_key),\n",
    "        options={\n",
    "            \"f_target\": -0.01,\n",
    "            \"maxiter\": 400,\n",
    "            \"maxfev\": 400,\n",
    "            \"feasibility_tol\": 1e-6,\n",
    "            \"initial_tr_radius\": np.sqrt(2*10),\n",
    "            \"final_tr_radius\": 1e-5,\n",
    "            \"disp\": False,\n",
    "            \"scale\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Minimization result:\\n\", res_mini)\n",
    "\n",
    "    # Get gld params and fp.\n",
    "    \n",
    "    s_x_min = otaf.common.bidirectional_string_to_array_conversion(res_mini.x)\n",
    "    s_x_max = otaf.common.bidirectional_string_to_array_conversion(res_maxi.x)\n",
    "    \n",
    "    if experiment_key :\n",
    "        gld_min = result_dict[experiment_key][s_x_min]['GLD_PARAMS']\n",
    "        gld_max = result_dict[experiment_key][s_x_max]['GLD_PARAMS']\n",
    "        fp_min = result_dict[experiment_key][s_x_min]['FP_GLD']\n",
    "        fp_max = result_dict[experiment_key][s_x_max]['FP_GLD']\n",
    "    else :\n",
    "        gld_min = result_dict[s_x_min]['GLD_PARAMS']\n",
    "        gld_max = result_dict[s_x_max]['GLD_PARAMS']\n",
    "        fp_min = result_dict[s_x_min]['FP_GLD']\n",
    "        fp_max = result_dict[s_x_max]['FP_GLD']\n",
    "\n",
    "    return (res_mini.x, res_maxi.x), (gld_min, gld_max), (fp_min, fp_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb89faf-31a3-42f5-860c-043667015974",
   "metadata": {},
   "source": [
    "## Maximize / Minimize Pf sor failure slack of s=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b7f9e8-f0c4-4e15-8e12-73526fc7bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a2e66-1537-492e-aec5-265470d78292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_x_000, res_gld_000, res_fp_000 = pf_min_max_optimizer(0.0, result_dict, \"experiment_slack00\")\n",
    "otaf.plotting.plot_gld_pbox_cdf(gld, *res_gld_000, np.linspace(-0.05,0.2,1000), xlabel=\"slack\", title=\"P-Box Slack Falure = 0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88adeb-00a8-46ab-a0d7-f5f413d5ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_x_005, res_gld_005, res_fp_005 = pf_min_max_optimizer(0.05, result_dict, \"experiment_slack005\")\n",
    "otaf.plotting.plot_gld_pbox_cdf(gld, *res_gld_005, np.linspace(-0.05,0.2,1000), xlabel=\"slack\", title=\"P-Box Slack Falure = 0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fba38e-5302-4675-bf28-17cbe1d19cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_x_010, res_gld_010, res_fp_010 = pf_min_max_optimizer(0.1, result_dict, \"experiment_slack010\")\n",
    "otaf.plotting.plot_gld_pbox_cdf(gld, *res_gld_010, np.linspace(-0.05,0.2,1000), xlabel=\"slack\", title=\"P-Box Slack Falure = 0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341fb861-d6b7-48a5-ae43-e899acc6fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_x_001, res_gld_001, res_fp_001 = pf_min_max_optimizer(0.01, result_dict, \"experiment_slack001\")\n",
    "otaf.plotting.plot_gld_pbox_cdf(gld, *res_gld_001, np.linspace(-0.05,0.2,1000), xlabel=\"slack\", title=\"P-Box Slack Falure = 0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2875c-dfed-46c6-9498-164c48b645f3",
   "metadata": {},
   "source": [
    "Let's clean up the result dictionary by removing the points where constraints are not respected (0.5% max deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fa2aea-0d1d-4b3d-a3f0-06cd0acec70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "otaf.plotting.plot_gld_pbox_cdf(gld, *res_gld_001, np.linspace(-0.05,0.2,1000), xlabel=\"slack\", title=\"P-Box Slack Falure = 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c2b7a-86ee-457a-8105-1422184d926f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdc000 = {key:val for key,val in result_dict[\"experiment_slack00\"].items() if ((np.abs(val.get('CONST',0.1))<=0.005).all() and 'GLD_PARAMS' in val.keys())}\n",
    "rdc005 = {key:val for key,val in result_dict[\"experiment_slack005\"].items() if ((np.abs(val.get('CONST',0.1))<=0.005).all() and 'GLD_PARAMS' in val.keys())}\n",
    "rdc010 = {key:val for key,val in result_dict[\"experiment_slack010\"].items() if ((np.abs(val.get('CONST',0.1))<=0.005).all() and 'GLD_PARAMS' in val.keys())}\n",
    "rdc = {}\n",
    "rdc.update(rdc000)\n",
    "rdc.update(rdc005)\n",
    "rdc.update(rdc010)\n",
    "gld_param_list = [val['GLD_PARAMS'] for key,val in rdc.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44559899-500c-4926-8186-102f1ee0db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "otaf.plotting.plot_gld_pbox_cdf2(gld, gld_param_list,np.linspace(-0.05,0.2,1000) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a673a4d-dfef-4e55-8f61-36fd6b02ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimization_function_mini(np.array([1,0,1,0.0])))\n",
    "print(optimization_function_mini(np.array([0.0,1.0,0.0,1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6b0bd-6be1-412b-85d1-46100d921e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assembly_constraint_no_mean(np.array([1,0,1,0.0])))\n",
    "print(assembly_constraint_no_mean(np.array([0.0,1.0,0.0,1.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac4d232-51df-47a2-be6f-cef3432991be",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1=[0.09499342, 0.03096709, 0.06325466, 0.25066254]\n",
    "g2=[0.09818191, 0.0458073,  0.50112086, 0.14145006]\n",
    "otaf.plotting.plot_gld_pbox_cdf(gld, g2, g1, np.linspace(-0.05,0.2,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5aeeb-1f6c-467d-b662-9ae356706e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
